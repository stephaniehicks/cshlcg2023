[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSHL Computational Genomics 2023",
    "section": "",
    "text": "Preface\nWelcome to the course material from Stephanie Hicks at the CSHL Computational Genomics 2023 course!"
  },
  {
    "objectID": "index.html#key-resources",
    "href": "index.html#key-resources",
    "title": "CSHL Computational Genomics 2023",
    "section": "Key resources",
    "text": "Key resources\n\nCourse material: https://stephaniehicks.com/cshlcg2023\nCode on GitHub: https://github.com/stephaniehicks/cshlcg2023"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "CSHL Computational Genomics 2023",
    "section": "Instructor",
    "text": "Instructor\nStephanie C. Hicks, PhD\nAssociate Professor of Biomedical Engineering and Biostatistics\nJohns Hopkins Malone Center for Engineering in Healthcare\nCenter for Computational Biology | Center for Imaging Science\nJohns Hopkins University\nhttps://www.stephaniehicks.com | shicks19@jhu.edu\nPronouns: she/her \n\nweb: https://www.stephaniehicks.com\nemail: shicks19@jhu.edu\ntwitter: @stephaniehicks"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "CSHL Computational Genomics 2023",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course website was developed and is maintained by Stephanie C. Hicks.\nThe course materials are licensed under the GPL-3 License. Linked and embedded materials are governed by their own licenses. I assume that all external materials used or embedded here are covered under the educational fair use policy. If this is not the case and any material displayed here violates copyright, please let me know and I will remove it."
  },
  {
    "objectID": "intro-to-r.html#acknowledgements",
    "href": "intro-to-r.html#acknowledgements",
    "title": "1  Introduction to R",
    "section": "1.1 Acknowledgements",
    "text": "1.1 Acknowledgements\nMaterial for this chapter was borrowed and adopted from\n\nhttps://rdpeng.github.io/Biostat776/lecture-introduction-and-overview.html\nhttps://rafalab.github.io/dsbook\nhttps://rmd4sci.njtierney.com\nhttps://andreashandel.github.io/MADAcourse\n\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\nAn overview and history of R from Roger Peng\nInstalling R and RStudio from Rafael Irizarry\nGetting Started in R and RStudio from Rafael Irizarry\nR for Data Science by Wickham & Grolemund (2017). Covers most of the basics of using R for data analysis.\nAdvanced R by Wickham (2014). Covers a number of areas including object-oriented, programming, functional programming, profiling and other advanced topics.\nRStudio IDE cheatsheet"
  },
  {
    "objectID": "intro-to-r.html#learning-objectives",
    "href": "intro-to-r.html#learning-objectives",
    "title": "1  Introduction to R",
    "section": "1.2 Learning objectives",
    "text": "1.2 Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nLearn about (some of) the history of R.\nIdentify some of the strengths and weaknesses of R.\nInstall R and Rstudio on your computer.\nKnow how to install and load R packages."
  },
  {
    "objectID": "intro-to-r.html#overview-and-history-of-r",
    "href": "intro-to-r.html#overview-and-history-of-r",
    "title": "1  Introduction to R",
    "section": "1.3 Overview and history of R",
    "text": "1.3 Overview and history of R\nBelow is a very quick introduction to R, to get you set up and running.\nLike every programming language, R has its advantages and disadvantages. If you search the internet, you will quickly discover lots of folks with opinions about R. Some of the features that are useful to know are:\n\nR is open-source, freely accessible, and cross-platform (multiple OS).\nR is a “high-level” programming language, relatively easy to learn.\n\nWhile “Low-level” programming languages (e.g. Fortran, C, etc) often have more efficient code, they can also be harder to learn because it is designed to be close to a machine language.\nIn contrast, high-level languages deal more with variables, objects, functions, loops, and other abstract CS concepts with a focus on usability over optimal program efficiency.\n\nR integrates easily with document preparation systems like LaTeX, but R files can also be used to create .docx, .pdf, .html, .ppt files with integrated R code output and graphics.\nThe R Community is very dynamic, helpful and welcoming.\n\nCheck out the #rstats or #rtistry on Twitter, TidyTuesday podcast and community activity in the R4DS Online Learning Community, and r/rstats subreddit.\nIf you are looking for more local resources, check out R-Ladies Baltimore.\n\nThrough R packages, it is easy to get lots of state-of-the-art algorithms.\nDocumentation and help files for R are generally good.\n\n\n1.3.1 Basic Features of R\nToday R runs on almost any standard computing platform and operating system.\nIts open source nature means that anyone is free to adapt the software to whatever platform they choose.\nOne nice feature that R shares with many popular open source projects is frequent releases.\n\nThese days there is a major annual release, typically in October, where major new features are incorporated and released to the public.\nThroughout the year, smaller-scale bugfix releases will be made as needed.\n\nAnother key advantage that R has over many other packages (even today) is its sophisticated graphics capabilities. R’s ability to create “publication quality” graphics has existed since the very beginning and has generally been better than competing packages. Today, with many more visualization packages available than before, that trend continues.\nFinally, one of the joys of using R has nothing to do with the language itself, but rather with the active and vibrant user community. In many ways, a language is successful inasmuch as it creates a platform with which many people can create new things.\nR is that platform and thousands of people around the world have come together to make contributions to R, to develop packages, and help each other use R for all kinds of applications.\nThe R-help and R-devel mailing lists have been highly active for over a decade now and there is considerable activity on web sites like Stack Overflow, Twitter #rstats, #rtistry, and Reddit.\n\n\n1.3.2 Free Software\nA major advantage that R has over many other statistical packages and is that it’s free in the sense of free software (it’s also free in the sense of free beer). The copyright for the primary source code for R is held by the R Foundation and is published under the GNU General Public License version 2.0.\nAccording to the Free Software Foundation, with free software, you are granted the following four freedoms\n\nThe freedom to run the program, for any purpose (freedom 0).\nThe freedom to study how the program works, and adapt it to your needs (freedom 1). Access to the source code is a precondition for this.\nThe freedom to redistribute copies so you can help your neighbor (freedom 2).\nThe freedom to improve the program, and release your improvements to the public, so that the whole community benefits (freedom 3). Access to the source code is a precondition for this.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can visit the Free Software Foundation’s web site to learn a lot more about free software. The Free Software Foundation was founded by Richard Stallman in 1985 and Stallman’s personal web site is an interesting read if you happen to have some spare time.\n\n\n\n\n1.3.3 Design of the R System\nThe primary R system is available from the Comprehensive R Archive Network, also known as CRAN. CRAN also hosts many add-on packages that can be used to extend the functionality of R.\nThe R system is divided into 2 conceptual parts:\n\nThe “base” R system that you download from CRAN:\n\n\nLinux\nWindows\nMac\n\n\nEverything else.\n\nR functionality is divided into a number of packages.\nPackages are installable pieces of software that contain functions for you use in your own data analyses.\n\nThe “base” R system contains, among other things, the base package which is required to run R and contains the most fundamental functions.\nThe other packages contained in the “base” system include utils, stats, datasets, graphics, grDevices, grid, methods, tools, parallel, compiler, splines, tcltk, stats4.\nThere are also “Recommended” packages: dplyr, ggplot2, etc.\n\nWhen you download a fresh installation of R from CRAN, you get all of the above, which represents a substantial amount of functionality. However, there are many other packages available:\n\nThere are over 10,000 packages on CRAN that have been developed by users and programmers around the world.\nThere are also many packages associated with the Bioconductor project.\nPeople often make packages available on their personal websites; there is no reliable way to keep track of how many packages are available in this fashion."
  },
  {
    "objectID": "intro-to-r.html#using-r-and-rstudio",
    "href": "intro-to-r.html#using-r-and-rstudio",
    "title": "1  Introduction to R",
    "section": "1.4 Using R and RStudio",
    "text": "1.4 Using R and RStudio\n\nIf R is the engine and bare bones of your car, then RStudio is like the rest of the car. The engine is super critical part of your car. But in order to make things properly functional, you need to have a steering wheel, comfy seats, a radio, rear and side view mirrors, storage, and seatbelts. — Nicholas Tierney\n\n[Source]\nThe RStudio layout has the following features:\n\nOn the upper left, something called a Rmarkdown script\nOn the lower left, the R console\nOn the lower right, the view for files, plots, packages, help, and viewer.\nOn the upper right, the environment / history pane\n\n\n\n\nA screenshot of the RStudio integrated developer environment (IDE) – aka the working environment\n\n\nThe R console is the bit where you can run your code. This is where the R code in your Rmarkdown document gets sent to run (we’ll learn about these files later).\nThe file/plot/pkg viewer is a handy browser for your current files, like Finder, or File Explorer, plots are where your plots appear, you can view packages, see the help files. And the environment / history pane contains the list of things you have created, and the past commands that you have run.\n\n1.4.1 Installing R and RStudio\n\nIf you have not already, install R first. If you already have R installed, make sure it is a fairly recent version, version 4.0 or newer. If yours is older, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it’s a recent version.\n\n\n\n\n\n\n\nTip\n\n\n\nInstalling R and RStudio should be fairly straightforward. However, a great set of detailed instructions is in Rafael Irizarry’s dsbook:\n\nhttps://rafalab.github.io/dsbook/installing-r-rstudio.html\n\n\n\nI personally only have experience with Mac, but everything should work on all the standard operating systems (Windows, Mac, and even Linux).\n\n\n1.4.2 RStudio default options\nTo first get set up, I highly recommend changing the following setting\nTools &gt; Global Options (or Cmd + , on macOS)\nUnder the General tab:\n\nFor workspace\n\nUncheck restore .RData into workspace at startup\nSave workspace to .RData on exit : “Never”\n\nFor History\n\nUncheck “Always save history (even when not saving .RData)\nUncheck “Remove duplicate entries in history”\n\n\nThis means that you won’t save the objects and other things that you create in your R session and reload them. This is important for two reasons\n\nReproducibility: you don’t want to have objects from last week cluttering your session\nPrivacy: you don’t want to save private data or other things to your session. You only want to read these in.\n\nYour “history” is the commands that you have entered into R.\nAdditionally, not saving your history means that you won’t be relying on things that you typed in the last session, which is a good habit to get into!\n\n\n1.4.3 Installing and loading R packages\nAs we discussed, most of the functionality and features in R come in the form of add-on packages. There are tens of thousands of packages available, some big, some small, some well documented, some not.\nThe “official” place for packages is the CRAN website. If you are interested in packages on a specific topic, the CRAN task views provide curated descriptions of packages sorted by topic.\nTo install an R package from CRAN, one can simply call the install.packages() function and pass the name of the package as an argument. For example, to install the ggplot2 package from CRAN: open RStudio,go to the R prompt (the &gt; symbol) in the lower-left corner and type\n\ninstall.packages(\"ggplot2\")\n\nand the appropriate version of the package will be installed.\nOften, a package needs other packages to work (called dependencies), and they are installed automatically. It usually does not matter if you use a single or double quotation mark around the name of the package.\n\n\n\n\n\n\nQuestions\n\n\n\n\nAs you installed the ggplot2 package, what other packages were installed?\nWhat happens if you tried to install GGplot2?\n\n\n\nIt could be that you already have all packages required by ggplot2 installed. In that case, you will not see any other packages installed. To see which of the packages above ggplot2 needs (and thus installs if it is not present), type into the R console:\n\ntools::package_dependencies(\"ggplot2\")\n\nIn RStudio, you can also install (and update/remove) packages by clicking on the ‘Packages’ tab in the bottom right window.\nIt is very common these days for packages to be developed on GitHub. It is possible to install packages from GitHub directly. Those usually contain the latest version of the package, with features that might not be available yet on the CRAN website. Sometimes, in early development stages, a package is only on GitHub until the developer(s) feel it is good enough for CRAN submission. So installing from GitHub gives you the latest. The downside is that packages under development can often be buggy and not working right. To install packages from GitHub, you need to install the remotes package and then use the following function\n\nremotes::install_github()\n\nYou only need to install a package once, unless you upgrade/re-install R. Once installed, you still need to load the package before you can use it. That has to happen every time you start a new R session. You do that using the library() command. For instance to load the ggplot2 package, type\n\nlibrary('ggplot2')\n\nYou may or may not see a short message on the screen. Some packages show messages when you load them, and others do not.\nThis was a quick overview of R packages. We will use a lot of them, so you will get used to them rather quickly.\n\n\n1.4.4 Getting started in RStudio\nWhile one can use R and do pretty much every task, including all the ones we cover in this class, without using RStudio, RStudio is very useful, has lots of features that make your R coding life easier and has become pretty much the default integrated development environment (IDE) for R. Since RStudio has lots of features, it takes time to learn them. A good resource to learn more about RStudio are the R Studio Essentials collection of videos.\n\n\n\n\n\n\nTip\n\n\n\nFor more information on setting up and getting started with R, RStudio, and R packages, read the Getting Started chapter in the dsbook:\n\nhttps://rafalab.github.io/dsbook/getting-started.html\n\nThis chapter gives some tips, shortcuts, and ideas that might be of interest even to those of you who already have R and/or RStudio experience."
  },
  {
    "objectID": "intro-to-r.html#session-info",
    "href": "intro-to-r.html#session-info",
    "title": "1  Introduction to R",
    "section": "1.5 Session Info",
    "text": "1.5 Session Info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.1    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.1       htmltools_0.5.6.1 rstudioapi_0.15.0 yaml_2.3.7       \n [9] rmarkdown_2.25    knitr_1.44        jsonlite_1.8.7    xfun_0.40        \n[13] digest_0.6.33     rlang_1.1.1       evaluate_0.22"
  },
  {
    "objectID": "exercises-01.html#tasks",
    "href": "exercises-01.html#tasks",
    "title": "Group work",
    "section": "Tasks",
    "text": "Tasks\nLet’s take 5 mins and pair up with another person. Pick out a few (or all!) of the questions below to ask each other and try to answer them."
  },
  {
    "objectID": "exercises-01.html#questions-about-r-and-rstudio",
    "href": "exercises-01.html#questions-about-r-and-rstudio",
    "title": "Group work",
    "section": "Questions about R and RStudio",
    "text": "Questions about R and RStudio\n\nWhat is an R package and what is it used for?\nWhat function in R can be used to install packages from CRAN?\nWhat is a limitation of the current R system?\nHow many packages are on CRAN? Where can you find this information?\nHow many packages are on GitHub? Is it possible to find this information?\nWhat is the craziest and/or coolest R package you have heard of?\nIf you had to describe a set of tasks that you would like to have in an R package, what would it be?"
  },
  {
    "objectID": "reproducible-res.html#acknowledgements",
    "href": "reproducible-res.html#acknowledgements",
    "title": "2  Reproducible Research",
    "section": "2.1 Acknowledgements",
    "text": "2.1 Acknowledgements\nMaterial for this chapter was borrowed and adopted from\n\nhttps://rdpeng.github.io/Biostat776/lecture-literate-statistical-programming.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown\nhttps://ropensci.github.io/reproducibility-guide/sections/introduction\nhttps://rdpeng.github.io/Biostat776\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\nRMarkdown Tips and Tricks by Indrajeet Patil\nhttps://bookdown.org/yihui/rmarkdown\nhttps://bookdown.org/yihui/rmarkdown-cookbook"
  },
  {
    "objectID": "reproducible-res.html#learning-objectives",
    "href": "reproducible-res.html#learning-objectives",
    "title": "2  Reproducible Research",
    "section": "2.2 Learning objectives",
    "text": "2.2 Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nKnow the difference between replication and reproducibility\nIdentify valid reasons why replication and/or reproducibility is not always possible\nIdentify key components to enable reproducible data analyses\nBe able to define literate programming\nRecognize differences between available tools to for literate programming\nKnow how to efficiently work within RStudio for efficient literate programming\nCreate a R Markdown document"
  },
  {
    "objectID": "reproducible-res.html#introduction",
    "href": "reproducible-res.html#introduction",
    "title": "2  Reproducible Research",
    "section": "2.3 Introduction",
    "text": "2.3 Introduction\nThis chapter will be about reproducible reporting, and I want to take the opportunity to cover some basic concepts and ideas that are related to reproducible reporting, just in case you have not heard about it or do not know what it is.\nBefore we get to reproducibility, we need to cover a little background with respect to how science works (even if you are not a scientist, this is important).\nThe ultimate standard in strengthening scientific evidence is replication. Assume you claim that X causes Y, or that Vitamin C improves disease. The goal of replication is to have independent people to do independent things with different data, different methods, and different laboratories and see if they get the same result.\nThere is a sense that if a relationship in nature is truly there, then it should be robust to having different people discover it in different ways. Replication is particularly important in areas where findings can have big policy impacts or can influence regulatory types of decisions.\n\n2.3.1 What is wrong with replication?\nThere is really nothing wrong with it. This is what science has been doing for a long time, through hundreds of years. And there is nothing wrong with it today.\nBut the problem is that it is becoming more and more challenging to do replication or to replicate other studies.\nHere are some reasons:\n\nOften times studies are much larger and more costly than previously. If you want to do ten versions of the same study, you need ten times as much money and there is not as much money around as there used to be.\nSometimes it is difficult to replicate a study because if the original study took 20 years to complete, it is difficult to wait around another 20 years for replication.\nSome studies are just plain unique, such as studying the impact of a massive earthquake in a very specific location and time. If you are looking at a unique situation in time or a unique population, you cannot readily replicate that situation.\n\nThere are a lot of good reasons why you cannot replicate a study. If you cannot replicate a study, is the alternative just to do nothing (?? 😱), just let that study stand by itself?\nThe idea behind a reproducible reporting is to create a kind of minimum standard (or a middle ground) where we will not be replicating a study, but maybe we can do something in between. What can we do that’s in between the gold standard and doing nothing?\nThat is where reproducibility comes in. That’s how we can kind of bridge the gap between replication and nothing.\nIn non-research settings, often full replication is not even the point. Often the goal is to preserve something to the point where anybody in an organization can repeat what you did (for example, after you leave the organization).\n\nIn this case, reproducibility is key to maintaining the history of a project and making sure that every step along the way is clear.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nReplication, whereby scientific questions are examined and verified independently by different scientists, is the gold standard for scientific validity.\nReplication can be difficult and often there are no resources to independently replicate a study.\nReproducibility, whereby data and code are re-analyzed by independent scientists to obtain the same results of the original investigator, is a reasonable minimum standard when replication is not possible."
  },
  {
    "objectID": "reproducible-res.html#reproducibility-to-the-rescue",
    "href": "reproducible-res.html#reproducibility-to-the-rescue",
    "title": "2  Reproducible Research",
    "section": "2.4 Reproducibility to the Rescue",
    "text": "2.4 Reproducibility to the Rescue\nLet’s first define reproducibility. The basic idea is that you need to make the data available for the original study and the computational methods available so that other people can look at your data and run the kind of analysis that you have run, and come to the same findings that you found.\nWhat reproducible reporting is about is a validation of the data analysis (not the original question itself). Because you are not collecting independent data using independent methods, it is a little bit more difficult to validate the scientific question itself. But if you can take someone’s data and reproduce their findings, then you can, in some sense, validate the data analysis.\nIn this way, you can at least have confidence that you can reproduce the analysis.\nRecently, there has been a lot of discussion of reproducibility in the media and in the scientific literature. For example, he journal Science had a special issue on reproducibility and data replication.\n\nhttps://www.science.org/toc/science/334/6060\n\n\n2.4.1 Why does this matter?\nHere is an example. In 2012, a feature on the TV show 60 minutes looked at a major incident at Duke University where many results involving a promising cancer test were found to be not reproducible. This led to a number of studies and clinical trials having to be stopped, followed by an investigation which is still ongoing.\n\n\n\n\n[Source on YouTube]\n\n\n2.4.2 Types of reproducibility\nWhat are the different kinds of reproducible research? Enabling reproducibility can be complicated, but by separating out some of the levels and degrees of reproducibility the problem can become more manageable because we can focus our efforts on what best suits our specific scientific domain. Victoria Stodden (2014), a prominent scholar on this topic, has identified some useful distinctions in reproducible research:\n\nComputational reproducibility: when detailed information is provided about code, software, hardware and implementation details.\nEmpirical reproducibility: when detailed information is provided about non-computational empirical scientific experiments and observations. In practice this is enabled by making data freely available, as well as details of how the data was collected.\nStatistical reproducibility: when detailed information is provided about the choice of statistical tests, model parameters, threshold values, etc. This mostly relates to pre-registration of study design to prevent p-value hacking and other manipulations.\n\n[Source]\n\n\n2.4.3 Elements of computational reproducibility\nWhat do we need for computational reproducibility? There are a variety of ways to talk about this, but one basic definition that we hae come up with is that there are four things that are required to make results reproducible:\n\nAnalytic data. The data that were used for the analysis that was presented should be available for others to access. This is different from the raw data because very often in a data analysis the raw data are not all used for the analysis, but rather some subset is used. It may be interesting to see the raw data but impractical to actually have it. Analytic data is key to examining the data analysis.\nAnalytic code. The analytic code is the code that was applied to the analytic data to produce the key results. This may be preprocessing code, regression modeling code, or really any other code used to produce the results from the analytic data.\nDocumentation. Documentation of that code and the data is very important.\nDistribution. Finally, there needs to be some standard means of distribution, so all this data in the code is easily accessible.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nReproducible reporting is about is a validation of the data analysis\nThere are multiple types of reproducibility\nThere are four elements to computational reproducibility"
  },
  {
    "objectID": "reproducible-res.html#x-to-computational-x",
    "href": "reproducible-res.html#x-to-computational-x",
    "title": "2  Reproducible Research",
    "section": "2.5 “X” to “Computational X”",
    "text": "2.5 “X” to “Computational X”\nWhat is driving this need for a “reproducibility middle ground” between replication and doing nothing?\nFor starters, there are a lot of new technologies on the scene and in many different fields of study including, biology, chemistry and environmental science. These technologies allow us to collect data at a much higher throughput so we end up with these very complex and very high dimensional data sets.\nThese datasets can be collected almost instantaneously compared to even just ten years ago—the technology has allowed us to create huge data sets at essentially the touch of a button. Furthermore, we the computing power to take existing (already huge) databases and merge them into even bigger and bigger databases. Finally, the massive increase in computing power has allowed us to implement more sophisticated and complex analysis routines.\nThe analyses themselves, the models that we fit and the algorithms that we run, are much much more complicated than they used to be. Having a basic understanding of these algorithms is difficult, even for a sophisticated person, and it is almost impossible to describe these algorithms with words alone.\nUnderstanding what someone did in a data analysis now requires looking at code and scrutinizing the computer programs that people used.\nThe bottom line with all these different trends is that for every field “X”, there is now “Computational X”. There’s computational biology, computational astronomy—whatever it is you want, there is a computational version of it."
  },
  {
    "objectID": "reproducible-res.html#literate-programming",
    "href": "reproducible-res.html#literate-programming",
    "title": "2  Reproducible Research",
    "section": "2.6 Literate programming",
    "text": "2.6 Literate programming\nOne basic idea to make writing reproducible reports easier is what’s known as literate statistical programming (or sometimes called literate statistical practice). This comes from the idea of literate programming in the area of writing computer programs.\nThe idea is to think of a report or a publication as a stream of text and code.\n\nThe text is readable by people and the code is readable by computers.\nThe analysis is described in a series of text and code chunks.\nEach kind of code chunk will do something like load some data or compute some results.\nEach text chunk will relay something in a human readable language.\n\nThere might also be presentation code that formats tables and figures and there’s article text that explains what’s going on around all this code. This stream of text and code is a literate statistical program or a literate statistical analysis.\n\n2.6.1 Weaving and Tangling\nLiterate programs by themselves are a bit difficult to work with, but they can be processed in two important ways.\nLiterate programs can be weaved to produce human readable documents like PDFs or HTML web pages, and they can tangled to produce machine-readable “documents”, or in other words, machine readable code.\nThe basic idea behind literate programming in order to generate the different kinds of output you might need, you only need a single source document—you can weave and tangle to get the rest.\nIn order to use a system like this you need a documentational language, that’s human readable, and you need a programming language that’s machine readable (or can be compiled/interpreted into something that’s machine readable).\n\n\n2.6.2 rmarkdown\nA common choice for literate programming with R code is to build documents based on Markdown language. A markdown file is a plain text file that is typically given the extension .md.. The rmarkdown R package takes a R Markdown file (.Rmd) and weaves together R code chunks like this:\n```{r plot1, height=4, width=5, eval=FALSE, echo=TRUE}\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n```\n\n\n\n\n\n\nTip\n\n\n\nThe best resource for learning about R Markdown this by Yihui Xie, J. J. Allaire, and Garrett Grolemund:\n\nhttps://bookdown.org/yihui/rmarkdown\n\nThe R Markdown Cookbook by Yihui Xie, Christophe Dervieux, and Emily Riederer is really good too:\n\nhttps://bookdown.org/yihui/rmarkdown-cookbook\n\nThe authors of the 2nd book describe the motivation for the 2nd book as:\n\n“However, we have received comments from our readers and publisher that it would be beneficial to provide more practical and relatively short examples to show the interesting and useful usage of R Markdown, because it can be daunting to find out how to achieve a certain task from the aforementioned reference book (put another way, that book is too dry to read). As a result, this cookbook was born.”\n\n\n\nBecause this is chapter is built in a .qmd file (which is very similar to a .Rmd file), let’s demonstrate how this work. I am going to change eval=FALSE to eval=TRUE.\n\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhy do we not see the back ticks ``` anymore in the code chunk above that made the plot?\nWhat do you think we should do if we want to have the code executed, but we want to hide the code that made it?\n\n\n\nBefore we leave this section, I find that there is quite a bit of terminology to understand the magic behind rmarkdown that can be confusing, so let’s break it down:\n\nPandoc. Pandoc is a command line tool with no GUI that converts documents (e.g. from number of different markup formats to many other formats, such as .doc, .pdf etc). It is completely independent from R (but does come bundled with RStudio).\nMarkdown (markup language). Markdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats. A markdown file is a plain text file that is typically given the extension .md. It is completely independent from R.\nmarkdown (R package). markdown is an R package which converts .md files into HTML. It is no longer recommended for use has been surpassed by rmarkdown (discussed below).\nR Markdown (markup language). R Markdown is an extension of the markdown syntax. R Markdown files are plain text files that typically have the file extension .Rmd.\nrmarkdown (R package). The R package rmarkdown is a library that uses pandoc to process and convert .Rmd files into a number of different formats. This core function is rmarkdown::render(). Note: this package only deals with the markdown language. If the input file is e.g. .Rhtml or .Rnw, then you need to use knitr prior to calling pandoc (see below).\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the R Markdown Quick Tour for more:\n\nhttps://rmarkdown.rstudio.com/authoring_quick_tour.html\n\n\n\n\n\n\nArtwork by Allison Horst on RMarkdown\n\n\n\n\n2.6.3 knitr\nOne of the alternative that has come up in recent times is something called knitr.\n\nThe knitr package for R takes a lot of these ideas of literate programming and updates and improves upon them.\nknitr still uses R as its programming language, but it allows you to mix other programming languages in.\nYou can also use a variety of documentation languages now, such as LaTeX, markdown and HTML.\nknitr was developed by Yihui Xie while he was a graduate student at Iowa State and it has become a very popular package for writing literate statistical programs.\n\nKnitr takes a plain text document with embedded code, executes the code and ‘knits’ the results back into the document.\nFor for example, it converts\n\nAn R Markdown (.Rmd) file into a standard markdown file (.md)\nAn .Rnw (Sweave) file into to .tex format.\nAn .Rhtml file into to .html.\n\nThe core function is knitr::knit() and by default this will look at the input document and try and guess what type it is e.g. Rnw, Rmd etc.\nThis core function performs three roles:\n\nA source parser, which looks at the input document and detects which parts are code that the user wants to be evaluated.\nA code evaluator, which evaluates this code\nAn output renderer, which writes the results of evaluation back to the document in a format which is interpretable by the raw output type. For instance, if the input file is an .Rmd, the output render marks up the output of code evaluation in .md format.\n\n\n\n\n\n\nConverting a Rmd file to many outputs using knitr and pandoc\n\n\n\n\n[Source]\nAs seen in the figure above, from there pandoc is used to convert e.g. a .md file into many other types of file formats into a .html, etc.\nSo in summary:\n\n“R Markdown stands on the shoulders of knitr and Pandoc. The former executes the computer code embedded in Markdown, and converts R Markdown to Markdown. The latter renders Markdown to the output format you want (such as PDF, HTML, Word, and so on).”\n\n[Source]"
  },
  {
    "objectID": "reproducible-res.html#create-and-knit-your-first-r-markdown-document",
    "href": "reproducible-res.html#create-and-knit-your-first-r-markdown-document",
    "title": "2  Reproducible Research",
    "section": "2.7 Create and Knit Your First R Markdown Document",
    "text": "2.7 Create and Knit Your First R Markdown Document\n\n\nWhen creating your first R Markdown document, in RStudio you can\n\nGo to File &gt; New File &gt; R Markdown…\nFeel free to edit the Title\nMake sure to select “Default Output Format” to be HTML\nClick “OK”. RStudio creates the R Markdown document and places some boilerplate text in there just so you can see how things are setup.\nClick the “Knit” button (or go to File &gt; Knit Document) to make sure you can create the HTML output\n\nIf you successfully knit your first R Markdown document, then congratulations!\n\n\n\n\n\nMission accomplished!"
  },
  {
    "objectID": "reproducible-res.html#tips-and-tricks-in-r-markdown-in-rstudio",
    "href": "reproducible-res.html#tips-and-tricks-in-r-markdown-in-rstudio",
    "title": "2  Reproducible Research",
    "section": "2.8 Tips and tricks in R Markdown in RStudio",
    "text": "2.8 Tips and tricks in R Markdown in RStudio\nHere are shortcuts and tips on efficiently using RStudio to improve how you write code.\n\n2.8.1 Run code\nIf you want to run a code chunk:\ncommand + Enter on Mac\nCtrl + Enter on Windows\n\n\n2.8.2 Insert a comment in R and R Markdown\nTo insert a comment:\ncommand + Shift + C on Mac\nCtrl + Shift + C on Windows\nThis shortcut can be used both for:\n\nR code when you want to comment your code. It will add a # at the beginning of the line\nfor text in R Markdown. It will add &lt;!-- and --&gt; around the text\n\nNote that if you want to comment more than one line, select all the lines you want to comment then use the shortcut. If you want to uncomment a comment, apply the same shortcut.\n\n\n2.8.3 Knit a R Markdown document\nYou can knit R Markdown documents by using this shortcut:\ncommand + Shift + K on Mac\nCtrl + Shift + K on Windows\n\n\n2.8.4 Code snippets\nCode snippets is usually a few characters long and is used as a shortcut to insert a common piece of code. You simply type a few characters then press Tab and it will complete your code with a larger code. Tab is then used again to navigate through the code where customization is required. For instance, if you type fun then press Tab, it will auto-complete the code with the required code to create a function:\nname &lt;- function(variables) {\n  \n}\nPressing Tab again will jump through the placeholders for you to edit it. So you can first edit the name of the function, then the variables and finally the code inside the function (try by yourself!).\nThere are many code snippets by default in RStudio. Here are the code snippets I use most often:\n\nlib to call library()\n\n\nlibrary(package)\n\n\nmat to create a matrix\n\n\nmatrix(data, nrow = rows, ncol = cols)\n\n\nif, el, and ei to create conditional expressions such as if() {}, else {} and else if () {}\n\n\nif (condition) {\n  \n}\n\nelse {\n  \n}\n\nelse if (condition) {\n  \n}\n\n\nfun to create a function\n\n\nname &lt;- function(variables) {\n  \n}\n\n\nfor to create for loops\n\n\nfor (variable in vector) {\n  \n}\n\n\nts to insert a comment with the current date and time (useful if you have very long code and share it with others so they see when it has been edited)\n\n\n# Tue Jan 21 20:20:14 2020 ------------------------------\n\nYou can see all default code snippets and add yours by clicking on Tools &gt; Global Options… &gt; Code (left sidebar) &gt; Edit Snippets…\n\n\n2.8.5 Ordered list in R Markdown\nIn R Markdown, when creating an ordered list such as this one:\n\nItem 1\nItem 2\nItem 3\n\nInstead of bothering with the numbers and typing\n1. Item 1\n2. Item 2\n3. Item 3\nyou can simply type\n1. Item 1\n1. Item 2\n1. Item 3\nfor the exact same result (try it yourself or check the code of this article!). This way you do not need to bother which number is next when creating a new item.\nTo go even further, any numeric will actually render the same result as long as the first item is the number you want to start from. For example, you could type:\n1. Item 1\n7. Item 2\n3. Item 3\nwhich renders\n\nItem 1\nItem 2\nItem 3\n\nHowever, I suggest always using the number you want to start from for all items because if you move one item at the top, the list will start with this new number. For instance, if we move 7. Item 2 from the previous list at the top, the list becomes:\n7. Item 2\n1. Item 1\n3. Item 3\nwhich incorrectly renders\n\nItem 2\nItem 1\nItem 3\n\n\n\n2.8.6 New code chunk in R Markdown\nWhen editing R Markdown documents, you will need to insert a new R code chunk many times. The following shortcuts will make your life easier:\ncommand + option + I on Mac (or command + alt + I depending on your keyboard)\nCtrl + ALT + I on Windows\n\n\n2.8.7 Reformat code\nA clear and readable code is always easier and faster to read (and look more professional when sharing it to collaborators). To automatically apply the most common coding guidelines such as white spaces, indents, etc., use:\ncmd + Shift + A on Mac\nCtrl + Shift + A on Windows\nSo for example the following code which does not respect the guidelines (and which is not easy to read):\n1+1\n  for(i in 1:10){if(!i%%2){next}\nprint(i)\n }\nbecomes much more neat and readable:\n1 + 1\nfor (i in 1:10) {\n  if (!i %% 2) {\n    next\n  }\n  print(i)\n}\n\n\n2.8.8 RStudio addins\nRStudio addins are extensions which provide a simple mechanism for executing advanced R functions from within RStudio. In simpler words, when executing an addin (by clicking a button in the Addins menu), the corresponding code is executed without you having to write the code. RStudio addins have the advantage that they allow you to execute complex and advanced code much more easily than if you would have to write it yourself.\n\n\n\n\n\n\nTip\n\n\n\nFor more information about RStudio addins, check out:\n\nhttps://rstudio.github.io/rstudioaddins\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown\n\n\n\n\n\n2.8.9 Others\nSimilar to many other programs, you can also use:\n\ncommand + Shift + N on Mac and Ctrl + Shift + N on Windows to open a new R Script\ncommand + S on Mac and Ctrl + S on Windows to save your current script or R Markdown document\n\nCheck out Tools –&gt; Keyboard Shortcuts Help to see a long list of these shortcuts."
  },
  {
    "objectID": "reproducible-res.html#session-info",
    "href": "reproducible-res.html#session-info",
    "title": "2  Reproducible Research",
    "section": "2.9 Session Info",
    "text": "2.9 Session Info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.4       cli_3.6.1         knitr_1.44        rlang_1.1.1      \n [5] xfun_0.40         emojifont_0.5.5   showtextdb_3.0    sysfonts_0.8.8   \n [9] generics_0.1.3    proto_1.0.0       jsonlite_1.8.7    glue_1.6.2       \n[13] colorspace_2.1-0  htmltools_0.5.6.1 scales_1.2.1      fansi_1.0.5      \n[17] rmarkdown_2.25    grid_4.3.1        evaluate_0.22     munsell_0.5.0    \n[21] tibble_3.2.1      fastmap_1.1.1     yaml_2.3.7        lifecycle_1.0.3  \n[25] compiler_4.3.1    dplyr_1.1.3       pkgconfig_2.0.3   htmlwidgets_1.6.2\n[29] rstudioapi_0.15.0 digest_0.6.33     R6_2.5.1          tidyselect_1.2.0 \n[33] utf8_1.2.4        showtext_0.9-6    pillar_1.9.0      magrittr_2.0.3   \n[37] tools_4.3.1       gtable_0.3.4      ggplot2_3.4.4"
  },
  {
    "objectID": "exercises-02.html#tasks",
    "href": "exercises-02.html#tasks",
    "title": "Group work",
    "section": "Tasks",
    "text": "Tasks\nLet’s take 5 mins and pair up with another person. Pick out a few of the questions below to ask each other and try to answer them."
  },
  {
    "objectID": "exercises-02.html#reproducible-research",
    "href": "exercises-02.html#reproducible-research",
    "title": "Group work",
    "section": "Reproducible research",
    "text": "Reproducible research\n\nWhat is the difference between replication and reproducible?\nWhy can replication be difficult to achieve? Why is reproducibility a reasonable minimum standard when replication is not possible?\nWhat is needed to reproduce the results of a data analysis?\nWhat is literate programming?\nWhat is knitr and how is different than other literate statistical programming tools?\nWhere can you find a list of other commands that help make your code writing more efficient in RStudio?"
  },
  {
    "objectID": "exercises-02.html#on-your-own-at-a-computer",
    "href": "exercises-02.html#on-your-own-at-a-computer",
    "title": "Group work",
    "section": "On your own at a computer",
    "text": "On your own at a computer\n\nOpen up RStudio, and follow these instructions to Create and Knit Your First R Markdown Document.\nTry out some of the Tips and tricks in R Markdown in RStudio."
  },
  {
    "objectID": "tidyverse.html#learning-objectives",
    "href": "tidyverse.html#learning-objectives",
    "title": "3  Tidyverse and data viz",
    "section": "3.1 Learning objectives",
    "text": "3.1 Learning objectives\n\nKnow difference between relative vs absolute paths\nUse modern R packages (readr) for reading and writing data in R\nUnderstand the advantages of a tibble and data.frame data objects in R\nLearn about the dplyr R package to manage data frames\nRecognize the key verbs to manage data frames in dplyr\nUse the “pipe” operator to combine verbs together\nBe able to build up layers of graphics using ggplot()"
  },
  {
    "objectID": "tidyverse.html#reading-and-writing-data",
    "href": "tidyverse.html#reading-and-writing-data",
    "title": "3  Tidyverse and data viz",
    "section": "3.2 Reading and writing data",
    "text": "3.2 Reading and writing data\nHere, we introduce ays to read and write data (e.g. .txt and .csv files) using base R functions as well as more modern R packages, such as readr, which is typically 10x faster than base R.\n\n3.2.1 Relative paths\nWhen you open up a .Rproj file, RStudio changes the path (location on your computer) to the .Rproj location.\nAfter opening up a .Rproj file, you can test this by\n\ngetwd()\n\nWhen you open up someone else’s R code or analysis, you might also see the\n\nsetwd()\n\nfunction being used which explicitly tells R to change the absolute path or absolute location of which directory to move into.\nFor example, say I want to clone a GitHub repo from Roger, which has 100 R script files, and in every one of those files at the top is:\n\nsetwd(\"C:\\Users\\Roger\\path\\only\\that\\Roger\\has\")\n\nThe problem is, if I want to use his code, I will need to go and hand-edit every single one of those paths (C:\\Users\\Roger\\path\\only\\that\\Roger\\has) to the path that I want to use on my computer or wherever I saved the folder on my computer (e.g. /Users/Stephanie/Documents/path/only/I/have).\n\nThis is an unsustainable practice.\nI can go in and manually edit the path, but this assumes I know how to set a working directory. Not everyone does.\n\nSo instead of absolute paths:\n\nsetwd(\"/Users/jtleek/data\")\nsetwd(\"~/Desktop/files/data\")\nsetwd(\"C:\\\\Users\\\\Michelle\\\\Downloads\")\n\nA better idea is to use relative paths with the here R package.\n\nIt will recognize the top-level directory of a Git repo and supports building all paths relative to that.\nFor more on project-oriented workflow suggestions, read this post from Jenny Bryan.\n\n\n\n3.2.2 The here package\nLet’s try using the here package.\n\nlibrary(here)\n\nhere() starts at /Users/stephaniehicks/Documents/github/teaching/cshlcg2023\n\nhere::here()\n\n[1] \"/Users/stephaniehicks/Documents/github/teaching/cshlcg2023\"\n\n\nThis function creates a path unique to my computer, but will also be unique to yours.\n\nlist.files(here::here())\n\n [1] \"_book\"                    \"_freeze\"                 \n [3] \"_quarto.yml\"              \"cover.png\"               \n [5] \"cshlcg2023.Rproj\"         \"data\"                    \n [7] \"exercises-01.html\"        \"exercises-01.qmd\"        \n [9] \"exercises-02.html\"        \"exercises-02.qmd\"        \n[11] \"exercises-03.qmd\"         \"exercises-04.qmd\"        \n[13] \"figures\"                  \"index.html\"              \n[15] \"index.qmd\"                \"intro-to-bioc.qmd\"       \n[17] \"intro-to-r.html\"          \"intro-to-r.qmd\"          \n[19] \"intro-to-single-cell.qmd\" \"references.bib\"          \n[21] \"references.qmd\"           \"reproducible-res_files\"  \n[23] \"reproducible-res.html\"    \"reproducible-res.qmd\"    \n[25] \"site_libs\"                \"tidyverse_files\"         \n[27] \"tidyverse.qmd\"            \"tidyverse.rmarkdown\"     \n\nlist.files(here(\"data\"))\n\n[1] \"asthma.rda\"         \"chicago.rds\"        \"spotify_songs.RDS\" \n[4] \"team_standings.csv\"\n\nlist.files(here(\"data\", \"team_standings.csv\"))\n\ncharacter(0)\n\n\nNow we see that using the here::here() function is a relative path (relative to the .Rproj file in our cshlcg2022 folder.\nNext, let’s use the here package to read in some data with the readr package.\n\n\n3.2.3 The readr package\nThe readr package is recently developed by posit (formerly RStudio) to deal with reading in large flat files quickly. The package provides replacements for functions like read.table() and read.csv(). The analogous functions in readr are read_table() and read_csv().\nThese functions are often much faster than their base R analogues and provide a few other nice features such as progress meters.\nFor example, the package includes a variety of functions in the read_*() family that allow you to read in data from different formats of flat files. The following table gives a guide to several functions in the read_*() family.\n\n\n\n\n\nreadr function\nUse\n\n\n\n\nread_csv()\nReads comma-separated file\n\n\nread_csv2()\nReads semicolon-separated file\n\n\nread_tsv()\nReads tab-separated file\n\n\nread_delim()\nGeneral function for reading delimited files\n\n\nread_fwf()\nReads fixed width files\n\n\nread_log()\nReads log files\n\n\n\n\n\nA typical call to read_csv() will look as follows.\n\nlibrary(readr)\nteams &lt;- read_csv(here(\"data\", \"team_standings.csv\"))\n\nRows: 32 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Team\ndbl (1): Standing\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nteams\n\n# A tibble: 32 × 2\n   Standing Team       \n      &lt;dbl&gt; &lt;chr&gt;      \n 1        1 Spain      \n 2        2 Netherlands\n 3        3 Germany    \n 4        4 Uruguay    \n 5        5 Argentina  \n 6        6 Brazil     \n 7        7 Ghana      \n 8        8 Paraguay   \n 9        9 Japan      \n10       10 Chile      \n# ℹ 22 more rows"
  },
  {
    "objectID": "tidyverse.html#data-frames-and-tibbles",
    "href": "tidyverse.html#data-frames-and-tibbles",
    "title": "3  Tidyverse and data viz",
    "section": "3.3 Data frames and tibbles",
    "text": "3.3 Data frames and tibbles\nThe data frame (or data.frame) is a key data structure in statistics and in R.\nThe basic structure of a data frame is that there is one observation per row and each column represents a variable, a measure, feature, or characteristic of that observation.\nGiven the importance of managing data frames, it is important that we have good tools for dealing with them.\nFor example, operations like filtering rows, re-ordering rows, and selecting columns, can often be tedious operations in R whose syntax is not very intuitive. The dplyr package in the tidyverse is designed to mitigate a lot of these problems and to provide a highly optimized set of routines specifically for dealing with data frames.\n\n3.3.1 Tibbles\nAnother type of data structure that we need to discuss is called the tibble! It’s best to think of tibbles as an updated and stylish version of the data.frame.\nTibbles are what tidyverse packages work with most seamlessly. Now, that does not mean tidyverse packages require tibbles.\nIn fact, they still work with data.frames, but the more you work with tidyverse and tidyverse-adjacent packages, the more you will see the advantages of using tibbles.\nBefore we go any further, tibbles are data frames, but they have some new bells and whistles to make your life easier.\n\n\n3.3.2 How tibbles differ from data.frame\nThere are a number of differences between tibbles and data.frames.\n\n\n\n\n\n\nNote\n\n\n\nTo see a full vignette about tibbles and how they differ from data.frame, you will want to execute vignette(\"tibble\") and read through that vignette.\n\n\nWe will summarize some of the most important points here:\n\nInput type remains unchanged - data.frame is notorious for treating strings as factors; this will not happen with tibbles\nVariable names remain unchanged - In base R, creating data.frames will remove spaces from names, converting them to periods or add “x” before numeric column names. Creating tibbles will not change variable (column) names.\nThere are no row.names() for a tibble - Tidy data requires that variables be stored in a consistent way, removing the need for row names.\nTibbles print first ten rows and columns that fit on one screen - Printing a tibble to screen will never print the entire huge data frame out. By default, it just shows what fits to your screen.\n\n\n\n3.3.3 as_tibble()\nSince many packages use the historical data.frame from base R, you will often find yourself in the situation that you have a data.frame and want to convert that data.frame to a tibble.\nTo do so, the as_tibble() function is exactly what you are looking for.\nFor the example, here we use a dataset (chicago.rds) containing air pollution and temperature data for the city of Chicago in the U.S.\nThe dataset is available in the /data repository. You can load the data into R using the readRDS() function.\n\nlibrary(here)\nchicago &lt;- readRDS(here(\"data\", \"chicago.rds\"))\n\nYou can see some basic characteristics of the dataset with the dim() and str() functions.\n\ndim(chicago)\n\n[1] 6940    8\n\nstr(chicago)\n\n'data.frame':   6940 obs. of  8 variables:\n $ city      : chr  \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num  31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num  31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date, format: \"1987-01-01\" \"1987-01-02\" ...\n $ pm25tmean2: num  NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num  34 NA 34.2 47 NA ...\n $ o3tmean2  : num  4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num  20 23.2 23.8 30.4 30.3 ...\n\n\nWe see this data structure is a data.frame with 6940 observations and 8 variables.\nTo convert this data.frame to a tibble you would use the following:\n\nstr(as_tibble(chicago))\n\ntibble [6,940 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:6940] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:6940] 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date[1:6940], format: \"1987-01-01\" \"1987-01-02\" ...\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ..."
  },
  {
    "objectID": "tidyverse.html#the-dplyr-package",
    "href": "tidyverse.html#the-dplyr-package",
    "title": "3  Tidyverse and data viz",
    "section": "3.4 The dplyr package",
    "text": "3.4 The dplyr package\nThe dplyr package was developed by Posit (formely RStudio) and is an optimized and distilled version of the older plyr package for data manipulation or wrangling.\nThe dplyr package does not provide any “new” functionality to R per se, in the sense that everything dplyr does could already be done with base R, but it greatly simplifies existing functionality in R.\nOne important contribution of the dplyr package is that it provides a “grammar” (in particular, verbs) for data manipulation and for operating on data frames.\nWith this grammar, you can sensibly communicate what it is that you are doing to a data frame that other people can understand (assuming they also know the grammar). This is useful because it provides an abstraction for data manipulation that previously did not exist.\nAnother useful contribution is that the dplyr functions are very fast, as many key operations are coded in C++.\n\n3.4.1 dplyr grammar\nSome of the key “verbs” provided by the dplyr package are\n\nselect(): return a subset of the columns of a data frame, using a flexible notation\nfilter(): extract a subset of rows from a data frame based on logical conditions\narrange(): reorder rows of a data frame\nrename(): rename variables in a data frame\nmutate(): add new variables/columns or transform existing variables\nsummarise() / summarize(): generate summary statistics of different variables in the data frame, possibly within strata\n%&gt;%: the “pipe” operator is used to connect multiple verb actions together into a pipeline\n\n\n\n\n\n\n\nNote\n\n\n\nThe dplyr package as a number of its own data types that it takes advantage of.\nFor example, there is a handy print() method that prevents you from printing a lot of data to the console. Most of the time, these additional data types are transparent to the user and do not need to be worried about.\n\n\n\n\n3.4.2 dplyr installation\nThe dplyr package is installed when you install and load the tidyverse meta-package.\n\nlibrary(tidyverse)\n\nYou may get some warnings when the package is loaded because there are functions in the dplyr package that have the same name as functions in other packages. For now you can ignore the warnings."
  },
  {
    "objectID": "tidyverse.html#dplyr-functions",
    "href": "tidyverse.html#dplyr-functions",
    "title": "3  Tidyverse and data viz",
    "section": "3.5 dplyr functions",
    "text": "3.5 dplyr functions\nAll of the functions that we will discuss here will have a few common characteristics. In particular,\n\nThe first argument is a data frame type object.\nThe subsequent arguments describe what to do with the data frame specified in the first argument, and you can refer to columns in the data frame directly (without using the $ operator, just use the column names).\nThe return result of a function is a new data frame.\nData frames must be properly formatted and annotated for this to all be useful. In particular, the data must be tidy. In short, there should be one observation per row, and each column should represent a feature or characteristic of that observation.\n\n\n3.5.1 select()\nWe will continue to use the chicago dataset containing air pollution and temperature data.\n\nchicago &lt;- as_tibble(chicago)\nstr(chicago)\n\ntibble [6,940 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:6940] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:6940] 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date[1:6940], format: \"1987-01-01\" \"1987-01-02\" ...\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\n\nThe select() function can be used to select columns of a data frame that you want to focus on.\n\n\n\n\n\n\nExample\n\n\n\nSuppose we wanted to take the first 3 columns only. There are a few ways to do this.\nWe could for example use numerical indices:\n\nnames(chicago)[1:3]\n\n[1] \"city\" \"tmpd\" \"dptp\"\n\n\nBut we can also use the names directly:\n\nsubset &lt;- select(chicago, city:dptp)\nhead(subset)\n\n# A tibble: 6 × 3\n  city   tmpd  dptp\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 chic   31.5  31.5\n2 chic   33    29.9\n3 chic   33    27.4\n4 chic   29    28.6\n5 chic   32    28.9\n6 chic   40    35.1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe : normally cannot be used with names or strings, but inside the select() function you can use it to specify a range of variable names.\nYou can also omit variables using the select() function by using the negative sign. With select() you can do\n\nselect(chicago, -(city:dptp))\n\nwhich indicates that we should include every variable except the variables city through dptp. The equivalent code in base R would be\n\ni &lt;- match(\"city\", names(chicago))\nj &lt;- match(\"dptp\", names(chicago))\nhead(chicago[, -(i:j)])\n\n\n\nNot super intuitive, right?\nThe select() function also allows a special syntax that allows you to specify variable names based on patterns. So, for example, if you wanted to keep every variable that ends with a “2”, we could do\n\nsubset &lt;- select(chicago, ends_with(\"2\"))\nstr(subset)\n\ntibble [6,940 × 4] (S3: tbl_df/tbl/data.frame)\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\n\nOr if we wanted to keep every variable that starts with a “d”, we could do\n\nsubset &lt;- select(chicago, starts_with(\"d\"))\nstr(subset)\n\ntibble [6,940 × 2] (S3: tbl_df/tbl/data.frame)\n $ dptp: num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date: Date[1:6940], format: \"1987-01-01\" \"1987-01-02\" ...\n\n\nYou can also use more general regular expressions if necessary. See the help page (?select) for more details.\n\n\n3.5.2 filter()\nThe filter() function is used to extract subsets of rows from a data frame. This function is similar to the existing subset() function in R but is quite a bit faster in my experience.\n\n\n\n\n\n\nExample\n\n\n\nSuppose we wanted to extract the rows of the chicago data frame where the levels of PM2.5 are greater than 30 (which is a reasonably high level), we could do\n\nchic.f &lt;- filter(chicago, pm25tmean2 &gt; 30)\nstr(chic.f)\n\ntibble [194 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:194] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:194] 23 28 55 59 57 57 75 61 73 78 ...\n $ dptp      : num [1:194] 21.9 25.8 51.3 53.7 52 56 65.8 59 60.3 67.1 ...\n $ date      : Date[1:194], format: \"1998-01-17\" \"1998-01-23\" ...\n $ pm25tmean2: num [1:194] 38.1 34 39.4 35.4 33.3 ...\n $ pm10tmean2: num [1:194] 32.5 38.7 34 28.5 35 ...\n $ o3tmean2  : num [1:194] 3.18 1.75 10.79 14.3 20.66 ...\n $ no2tmean2 : num [1:194] 25.3 29.4 25.3 31.4 26.8 ...\n\n\nYou can see that there are now only 194 rows in the data frame and the distribution of the pm25tmean2 values is.\n\nsummary(chic.f$pm25tmean2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.05   32.12   35.04   36.63   39.53   61.50 \n\n\n\n\nWe can place an arbitrarily complex logical sequence inside of filter(), so we could for example extract the rows where PM2.5 is greater than 30 and temperature is greater than 80 degrees Fahrenheit.\n\nchic.f &lt;- filter(chicago, pm25tmean2 &gt; 30 & tmpd &gt; 80)\nselect(chic.f, date, tmpd, pm25tmean2)\n\n# A tibble: 17 × 3\n   date        tmpd pm25tmean2\n   &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 1998-08-23    81       39.6\n 2 1998-09-06    81       31.5\n 3 2001-07-20    82       32.3\n 4 2001-08-01    84       43.7\n 5 2001-08-08    85       38.8\n 6 2001-08-09    84       38.2\n 7 2002-06-20    82       33  \n 8 2002-06-23    82       42.5\n 9 2002-07-08    81       33.1\n10 2002-07-18    82       38.8\n11 2003-06-25    82       33.9\n12 2003-07-04    84       32.9\n13 2005-06-24    86       31.9\n14 2005-06-27    82       51.5\n15 2005-06-28    85       31.2\n16 2005-07-17    84       32.7\n17 2005-08-03    84       37.9\n\n\nNow there are only 17 observations where both of those conditions are met.\nOther logical operators you should be aware of include:\n\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\n\n\n\n\n==\nEquals\ncity == chic\n\n\n!=\nDoes not equal\ncity != chic\n\n\n&gt;\nGreater than\ntmpd &gt; 32.0\n\n\n&gt;=\nGreater than or equal to\ntmpd &gt;- 32.0\n\n\n&lt;\nLess than\ntmpd &lt; 32.0\n\n\n&lt;=\nLess than or equal to\ntmpd &lt;= 32.0\n\n\n%in%\nIncluded in\ncity %in% c(\"chic\", \"bmore\")\n\n\nis.na()\nIs a missing value\nis.na(pm10tmean2)\n\n\n\nNote: If you are ever unsure of how to write a logical statement, but know how to write its opposite, you can use the ! operator to negate the whole statement.\nA common use of this is to identify observations with non-missing data (e.g., !(is.na(pm10tmean2))).\n\n\n3.5.3 arrange()\nThe arrange() function is used to reorder rows of a data frame according to one of the variables/columns. Reordering rows of a data frame (while preserving corresponding order of other columns) is normally a pain to do in R. The arrange() function simplifies the process quite a bit.\nHere we can order the rows of the data frame by date, so that the first row is the earliest (oldest) observation and the last row is the latest (most recent) observation.\n\nchicago &lt;- arrange(chicago, date)\n\nWe can now check the first few rows\n\nhead(select(chicago, date, pm25tmean2), 3)\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  &lt;date&gt;          &lt;dbl&gt;\n1 1987-01-01         NA\n2 1987-01-02         NA\n3 1987-01-03         NA\n\n\nand the last few rows.\n\ntail(select(chicago, date, pm25tmean2), 3)\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  &lt;date&gt;          &lt;dbl&gt;\n1 2005-12-29       7.45\n2 2005-12-30      15.1 \n3 2005-12-31      15   \n\n\nColumns can be arranged in descending order too by useing the special desc() operator.\n\nchicago &lt;- arrange(chicago, desc(date))\n\nLooking at the first three and last three rows shows the dates in descending order.\n\nhead(select(chicago, date, pm25tmean2), 3)\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  &lt;date&gt;          &lt;dbl&gt;\n1 2005-12-31      15   \n2 2005-12-30      15.1 \n3 2005-12-29       7.45\n\ntail(select(chicago, date, pm25tmean2), 3)\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  &lt;date&gt;          &lt;dbl&gt;\n1 1987-01-03         NA\n2 1987-01-02         NA\n3 1987-01-01         NA\n\n\n\n\n3.5.4 rename()\nRenaming a variable in a data frame in R is surprisingly hard to do! The rename() function is designed to make this process easier.\nHere you can see the names of the first five variables in the chicago data frame.\n\nhead(chicago[, 1:5], 3)\n\n# A tibble: 3 × 5\n  city   tmpd  dptp date       pm25tmean2\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;          &lt;dbl&gt;\n1 chic     35  30.1 2005-12-31      15   \n2 chic     36  31   2005-12-30      15.1 \n3 chic     35  29.4 2005-12-29       7.45\n\n\nThe dptp column is supposed to represent the dew point temperature and the pm25tmean2 column provides the PM2.5 data.\nHowever, these names are pretty obscure or awkward and probably be renamed to something more sensible.\n\nchicago &lt;- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)\nhead(chicago[, 1:5], 3)\n\n# A tibble: 3 × 5\n  city   tmpd dewpoint date        pm25\n  &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;\n1 chic     35     30.1 2005-12-31 15   \n2 chic     36     31   2005-12-30 15.1 \n3 chic     35     29.4 2005-12-29  7.45\n\n\nThe syntax inside the rename() function is to have the new name on the left-hand side of the = sign and the old name on the right-hand side.\n\n\n3.5.5 mutate()\nThe mutate() function exists to compute transformations of variables in a data frame. Often, you want to create new variables that are derived from existing variables and mutate() provides a clean interface for doing that.\nFor example, with air pollution data, we often want to detrend the data by subtracting the mean from the data.\n\nThat way we can look at whether a given day’s air pollution level is higher than or less than average (as opposed to looking at its absolute level).\n\nHere, we create a pm25detrend variable that subtracts the mean from the pm25 variable.\n\nchicago &lt;- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE))\nhead(chicago)\n\n# A tibble: 6 × 9\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n  &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 chic     35     30.1 2005-12-31 15          23.5     2.53      13.2\n2 chic     36     31   2005-12-30 15.1        19.2     3.03      22.8\n3 chic     35     29.4 2005-12-29  7.45       23.5     6.79      20.0\n4 chic     37     34.5 2005-12-28 17.8        27.5     3.26      19.3\n5 chic     40     33.6 2005-12-27 23.6        27       4.47      23.5\n6 chic     35     29.6 2005-12-26  8.4         8.5    14.0       16.8\n# ℹ 1 more variable: pm25detrend &lt;dbl&gt;\n\n\nThere is also the related transmute() function, which does the same thing as mutate() but then drops all non-transformed variables.\nHere, we de-trend the PM10 and ozone (O3) variables.\n\nhead(transmute(chicago, \n               pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE),\n               o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE)))\n\n# A tibble: 6 × 2\n  pm10detrend o3detrend\n        &lt;dbl&gt;     &lt;dbl&gt;\n1      -10.4     -16.9 \n2      -14.7     -16.4 \n3      -10.4     -12.6 \n4       -6.40    -16.2 \n5       -6.90    -15.0 \n6      -25.4      -5.39\n\n\nNote that there are only two columns in the transmuted data frame.\n\n\n3.5.6 group_by()\nThe group_by() function is used to generate summary statistics from the data frame within strata defined by a variable.\nFor example, in this air pollution dataset, you might want to know what the average annual level of PM2.5 is?\nSo the stratum is the year, and that is something we can derive from the date variable.\nIn conjunction with the group_by() function, we often use the summarize() function (or summarise() for some parts of the world).\nNote: The general operation here is a combination of\n\nSplitting a data frame into separate pieces defined by a variable or group of variables (group_by())\nThen, applying a summary function across those subsets (summarize())\n\n\n\n\n\n\n\nExample\n\n\n\nFirst, we can create a year variable using as.POSIXlt().\n\nchicago &lt;- mutate(chicago, year = as.POSIXlt(date)$year + 1900)\n\nNow we can create a separate data frame that splits the original data frame by year.\n\nyears &lt;- group_by(chicago, year)\n\nFinally, we compute summary statistics for each year in the data frame with the summarize() function.\n\nsummarize(years, \n          pm25 = mean(pm25, na.rm = TRUE), \n          o3 = max(o3tmean2, na.rm = TRUE), \n          no2 = median(no2tmean2, na.rm = TRUE))\n\n# A tibble: 19 × 4\n    year  pm25    o3   no2\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1987 NaN    63.0  23.5\n 2  1988 NaN    61.7  24.5\n 3  1989 NaN    59.7  26.1\n 4  1990 NaN    52.2  22.6\n 5  1991 NaN    63.1  21.4\n 6  1992 NaN    50.8  24.8\n 7  1993 NaN    44.3  25.8\n 8  1994 NaN    52.2  28.5\n 9  1995 NaN    66.6  27.3\n10  1996 NaN    58.4  26.4\n11  1997 NaN    56.5  25.5\n12  1998  18.3  50.7  24.6\n13  1999  18.5  57.5  24.7\n14  2000  16.9  55.8  23.5\n15  2001  16.9  51.8  25.1\n16  2002  15.3  54.9  22.7\n17  2003  15.2  56.2  24.6\n18  2004  14.6  44.5  23.4\n19  2005  16.2  58.8  22.6\n\n\nsummarize() returns a data frame with year as the first column, and then the annual summary statistics of pm25, o3, and no2.\n\n\n\n\n3.5.7 %&gt;%\nThe pipeline operator %&gt;% is very handy for stringing together multiple dplyr functions in a sequence of operations.\nNotice above that every time we wanted to apply more than one function, the sequence gets buried in a sequence of nested function calls that is difficult to read, i.e.\n\nthird(second(first(x)))\n\nThis nesting is not a natural way to think about a sequence of operations.\nThe %&gt;% operator allows you to string operations in a left-to-right fashion, i.e.\n\nfirst(x) %&gt;% second %&gt;% third\n\n\n\n\n\n\n\nExample\n\n\n\nTake the example that we just did in the last section, but here use the pipe operator in a single expression.\n\nmutate(chicago, year = as.POSIXlt(date)$year + 1900) %&gt;%    \n        group_by(year) %&gt;% \n        summarize(years, pm25 = mean(pm25, na.rm = TRUE), \n          o3 = max(o3tmean2, na.rm = TRUE), \n          no2 = median(no2tmean2, na.rm = TRUE))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 131,860 × 12\n# Groups:   year [19]\n    year city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1  2005 chic     35     30.1 2005-12-31  16.2       23.5     2.53      13.2\n 2  2005 chic     36     31   2005-12-30  16.2       19.2     3.03      22.8\n 3  2005 chic     35     29.4 2005-12-29  16.2       23.5     6.79      20.0\n 4  2005 chic     37     34.5 2005-12-28  16.2       27.5     3.26      19.3\n 5  2005 chic     40     33.6 2005-12-27  16.2       27       4.47      23.5\n 6  2005 chic     35     29.6 2005-12-26  16.2        8.5    14.0       16.8\n 7  2005 chic     35     32.1 2005-12-25  16.2        8      14.4       13.8\n 8  2005 chic     37     35.2 2005-12-24  16.2       25.2     1.77      32.0\n 9  2005 chic     41     32.6 2005-12-23  16.2       34.5     6.91      29.1\n10  2005 chic     22     23.3 2005-12-22  16.2       42.5     5.39      33.7\n# ℹ 131,850 more rows\n# ℹ 3 more variables: pm25detrend &lt;dbl&gt;, o3 &lt;dbl&gt;, no2 &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above code, I pass the chicago data frame to the first call to mutate(), but then afterwards I do not have to pass the first argument to group_by() or summarize().\n\nOnce you travel down the pipeline with %&gt;%, the first argument is taken to be the output of the previous element in the pipeline.\n\n\n\n\n\n3.5.8 slice_*()\nThe slice_sample() function of the dplyr package will allow you to see a sample of random rows in random order.\nThe number of rows to show is specified by the n argument.\n\nThis can be useful if you do not want to print the entire tibble, but you want to get a greater sense of the values.\nThis is a good option for data analysis reports, where printing the entire tibble would not be appropriate if the tibble is quite large.\n\n\n\n\n\n\n\nExample\n\n\n\n\nslice_sample(chicago, n = 10)\n\n# A tibble: 10 × 10\n   city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 chic   57       47.5 1990-09-16  NA         10.5    20.4       11.8\n 2 chic   47.5     23.2 1988-04-12  NA         38      27.7       20.8\n 3 chic   77       55.2 1998-07-29  16         20      34.1       15.2\n 4 chic   66       50.7 1999-10-30  NA         68.5    38.7       23.1\n 5 chic   42.5     40.6 1996-12-23  NA         17.6     2.07      25.0\n 6 chic   68.5     57.8 1990-07-23  NA         36.5    20.2       26.3\n 7 chic   77.5     70.4 1994-08-24  NA         59.4    33.7       31.6\n 8 chic   64       46.9 1998-05-21   5.7       26      26.7       21.8\n 9 chic   79       65.4 2003-07-03  24.0       71.5    35.3       34.7\n10 chic   57       49.1 2001-11-14  38.9       95       9.18      38.6\n# ℹ 2 more variables: pm25detrend &lt;dbl&gt;, year &lt;dbl&gt;\n\n\n\n\nYou can also use slice_head() or slice_tail() to take a look at the top rows or bottom rows of your tibble. Again the number of rows can be specified with the n argument.\nThis will show the first 5 rows.\n\nslice_head(chicago, n = 5)\n\n# A tibble: 5 × 10\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n  &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 chic     35     30.1 2005-12-31 15          23.5     2.53      13.2\n2 chic     36     31   2005-12-30 15.1        19.2     3.03      22.8\n3 chic     35     29.4 2005-12-29  7.45       23.5     6.79      20.0\n4 chic     37     34.5 2005-12-28 17.8        27.5     3.26      19.3\n5 chic     40     33.6 2005-12-27 23.6        27       4.47      23.5\n# ℹ 2 more variables: pm25detrend &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nThis will show the last 5 rows.\n\nslice_tail(chicago, n = 5)\n\n# A tibble: 5 × 10\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n  &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 chic   32       28.9 1987-01-05    NA       NA       4.75      30.3\n2 chic   29       28.6 1987-01-04    NA       47       4.38      30.4\n3 chic   33       27.4 1987-01-03    NA       34.2     3.33      23.8\n4 chic   33       29.9 1987-01-02    NA       NA       3.30      23.2\n5 chic   31.5     31.5 1987-01-01    NA       34       4.25      20.0\n# ℹ 2 more variables: pm25detrend &lt;dbl&gt;, year &lt;dbl&gt;"
  },
  {
    "objectID": "tidyverse.html#the-ggplot2-plotting-system",
    "href": "tidyverse.html#the-ggplot2-plotting-system",
    "title": "3  Tidyverse and data viz",
    "section": "3.6 The ggplot2 Plotting System",
    "text": "3.6 The ggplot2 Plotting System\nIn this section, we will get into a little more of the nitty gritty of how ggplot2 builds plots and how you can customize various aspects of any plot.\n\n3.6.1 Basic components of a ggplot2 plot\nA ggplot2 plot consists of a number of key components.\n\nA data frame: stores all of the data that will be displayed on the plot\naesthetic mappings: describe how data are mapped to color, size, shape, location\ngeoms: geometric objects like points, lines, shapes\nfacets: describes how conditional/panel plots should be constructed\nstats: statistical transformations like binning, quantiles, smoothing\nscales: what scale an aesthetic map uses (example: left-handed = red, right-handed = blue)\ncoordinate system: describes the system in which the locations of the geoms will be drawn\n\nIt is essential to organize your data into a data frame before you start with ggplot2 (and all the appropriate metadata so that your data frame is self-describing and your plots will be self-documenting).\nWhen building plots in ggplot2 (rather than using qplot()), the “artist’s palette” model may be the closest analogy.\nEssentially, you start with some raw data, and then you gradually add bits and pieces to it to create a plot.\n\n\n\n\n\n\nNote\n\n\n\nPlots are built up in layers, with the typically ordering being\n\nPlot the data\nOverlay a summary\nAdd metadata and annotation\n\nFor quick exploratory plots you may not get past step 1."
  },
  {
    "objectID": "tidyverse.html#building-up-in-layers",
    "href": "tidyverse.html#building-up-in-layers",
    "title": "3  Tidyverse and data viz",
    "section": "3.7 Building up in layers",
    "text": "3.7 Building up in layers\nFirst, we can create a ggplot object that stores the dataset and the basic aesthetics for mapping the x- and y-coordinates for the plot.\n\n3.7.1 palmerpenguins dataset\nHere we will use the palmerpenguins dataset. These data contain observations for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n\n\n\n\n\nPalmer penguins\n\n\n\n\n[Source: Artwork by Allison Horst]\n\nlibrary(palmerpenguins)\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nIf we wanted to count the number of penguins for each of the three species, we can use the count() function in dplyr:\n\npenguins %&gt;% \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nFor example, we see there are a total of 152 Adelie penguins in the palmerpenguins dataset.\n\ng &lt;- ggplot(penguins, aes(x = flipper_length_mm, \n                          y = bill_length_mm))\nsummary(g)\n\ndata: species, island, bill_length_mm, bill_depth_mm,\n  flipper_length_mm, body_mass_g, sex, year [344x8]\nmapping:  x = ~flipper_length_mm, y = ~bill_length_mm\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n\nclass(g)\n\n[1] \"gg\"     \"ggplot\"\n\n\nYou can see above that the object g contains the dataset penguins and the mappings.\nNow, normally if you were to print() a ggplot object a plot would appear on the plot device, however, our object g actually does not contain enough information to make a plot yet.\n\ng &lt;- penguins %&gt;% \n      ggplot(aes(x = flipper_length_mm, \n                 y = bill_length_mm))\nprint(g)\n\n\n\n\nNothing to see here!\n\n\n\n\n\n\n3.7.2 First plot with point layer\nTo make a scatter plot, we need add at least one geom, such as points.\nHere, we add the geom_point() function to create a traditional scatter plot.\n\ng &lt;- penguins %&gt;% \n      ggplot(aes(x = flipper_length_mm, \n                 y = bill_length_mm))\ng + geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nHow does ggplot know what points to plot? In this case, it can grab them from the data frame penguins that served as the input into the ggplot() function.\n\n\n3.7.3 Adding more layers\n\n3.7.3.1 smooth\nBecause the data appear rather noisy, it might be better if we added a smoother on top of the points to see if there is a trend in the data with PM2.5.\n\ng + \n  geom_point() + \n  geom_smooth()\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nScatterplot with smoother\n\n\n\n\nThe default smoother is a loess smoother, which is flexible and nonparametric but might be too flexible for our purposes. Perhaps we’d prefer a simple linear regression line to highlight any first order trends. We can do this by specifying method = \"lm\" to geom_smooth().\n\ng + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nHere, we can see there appears to be a increasing trend.\nWe can color the points by species and a smoother by adding a linear regression.\n\npenguins %&gt;% \n  ggplot(aes(x = flipper_length_mm, \n             y = bill_length_mm, \n             color = species)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n3.7.3.2 facets\nWe can also stratify the scatter plot by another variable (e.g. sex) by adding a facet_grid() or facet_wrap() function.\n\npenguins %&gt;% \n  filter(!is.na(sex)) %&gt;% \n  ggplot(aes(x = flipper_length_mm, \n             y = bill_length_mm, \n             color = species)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  facet_grid(.~sex)\n\n\n\n\n\n\n3.7.3.3 Changing the theme\nThe default theme for ggplot2 uses the gray background with white grid lines.\nIf you don’t find this suitable, you can use the black and white theme by using the theme_bw() function.\nThe theme_bw() function also allows you to set the typeface for the plot, in case you don’t want the default Helvetica. Here we change the typeface to Times.\nFor things that only make sense globally, use theme(), i.e. theme(legend.position = \"none\"). Two standard appearance themes are included\n\ntheme_gray(): The default theme (gray background)\ntheme_bw(): More stark/plain\n\n\ng + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  theme_bw(base_family = \"Times\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n3.7.3.4 Modifying labels\nThere are a variety of annotations you can add to a plot, including different kinds of labels.\n\nxlab() for x-axis labels\nylab() for y-axis labels\nggtitle() for specifying plot titles\n\nlabs() function is generic and can be used to modify multiple types of labels at once\n:::\nHere is an example of modifying the title and the x and y labels to make the plot a bit more informative.\n\ng + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  labs(title = \"Palmer penguins\", \n       x = \"flipper length\", \n       y = \"bill length\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nModifying plot labels"
  },
  {
    "objectID": "tidyverse.html#session-info",
    "href": "tidyverse.html#session-info",
    "title": "3  Tidyverse and data viz",
    "section": "3.8 Session Info",
    "text": "3.8 Session Info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] palmerpenguins_0.1.1 here_1.0.1           lubridate_1.9.3     \n [4] forcats_1.0.0        stringr_1.5.0        dplyr_1.1.3         \n [7] purrr_1.0.2          readr_2.1.4          tidyr_1.3.0         \n[10] tibble_3.2.1         ggplot2_3.4.4        tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4        generics_0.1.3    lattice_0.22-5    stringi_1.7.12   \n [5] hms_1.1.3         digest_0.6.33     magrittr_2.0.3    evaluate_0.22    \n [9] grid_4.3.1        timechange_0.2.0  fastmap_1.1.1     Matrix_1.6-1.1   \n[13] rprojroot_2.0.3   jsonlite_1.8.7    mgcv_1.9-0        fansi_1.0.5      \n[17] scales_1.2.1      cli_3.6.1         rlang_1.1.1       crayon_1.5.2     \n[21] splines_4.3.1     bit64_4.0.5       munsell_0.5.0     withr_2.5.2      \n[25] yaml_2.3.7        tools_4.3.1       parallel_4.3.1    tzdb_0.4.0       \n[29] colorspace_2.1-0  vctrs_0.6.4       R6_2.5.1          lifecycle_1.0.3  \n[33] htmlwidgets_1.6.2 bit_4.0.5         vroom_1.6.4       pkgconfig_2.0.3  \n[37] pillar_1.9.0      gtable_0.3.4      glue_1.6.2        xfun_0.40        \n[41] tidyselect_1.2.0  rstudioapi_0.15.0 knitr_1.44        farver_2.1.1     \n[45] nlme_3.1-163      htmltools_0.5.6.1 rmarkdown_2.25    labeling_0.4.3   \n[49] compiler_4.3.1"
  },
  {
    "objectID": "exercises-03.html#overview",
    "href": "exercises-03.html#overview",
    "title": "4  Workshop",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nIn this workshop, you will explore spotify songs!\nPlease write up your solution using R Markdown and knitr. Please show all your code for each of the answers to each part.\nAt the end of the workshop, we will go over the answers."
  },
  {
    "objectID": "exercises-03.html#data",
    "href": "exercises-03.html#data",
    "title": "4  Workshop",
    "section": "4.2 Data",
    "text": "4.2 Data\nThat data for this part of the assignment comes from TidyTuesday, which is a weekly podcast and global community activity brought to you by the R4DS Online Learning Community. The goal of TidyTuesday is to help R learners learn in real-world contexts.\n\n\n\n\n\nIcon from TidyTuesday\n\n\n\n\n[Source: TidyTuesday]\nTo access the data, you need to install the tidytuesdayR R package and use the function tt_load() with the date of ‘2020-01-21’ to load the data.\n\ninstall.packages(\"tidytuesdayR\")\n\nThis is how you can download the data.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2020-01-21')\nspotify_songs &lt;- tuesdata$spotify_songs\n\nHowever, if you use this code, you will hit an API limit after trying to compile the document a few times. Instead, I suggest you use the following code below. Here, I provide the code below for you to avoid re-downloading data:\n\nlibrary(here)\nlibrary(tidyverse)\n\n# tests if a directory named \"data\" exists locally\nif(!dir.exists(here(\"data\"))) { dir.create(here(\"data\")) }\n\n# saves data only once (not each time you knit a R Markdown)\nif(!file.exists(here(\"data\",\"spotify_songs.RDS\"))) {\n  url_csv &lt;- 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv'\n  spotify_songs &lt;- readr::read_csv(url_csv)\n  \n  # save the file to RDS objects\n  saveRDS(spotify_songs, file= here(\"data\",\"spotify_songs.RDS\"))\n}\n\nHere we read in the .RDS dataset locally from our computing environment:\n\nspotify_songs &lt;- readRDS(here(\"data\",\"spotify_songs.RDS\"))\nas_tibble(spotify_songs)\n\n# A tibble: 32,833 × 23\n   track_id              track_name track_artist track_popularity track_album_id\n   &lt;chr&gt;                 &lt;chr&gt;      &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;         \n 1 6f807x0ima9a1j3VPbc7… I Don't C… Ed Sheeran                 66 2oCs0DGTsRO98…\n 2 0r7CVbZTWZgbTCYdfa2P… Memories … Maroon 5                   67 63rPSO264uRjW…\n 3 1z1Hg7Vb0AhHDiEmnDE7… All the T… Zara Larsson               70 1HoSmj2eLcsrR…\n 4 75FpbthrwQmzHlBJLuGd… Call You … The Chainsm…               60 1nqYsOef1yKKu…\n 5 1e8PAfcKUYoKkxPhrHqw… Someone Y… Lewis Capal…               69 7m7vv9wlQ4i0L…\n 6 7fvUMiyapMsRRxr07cU8… Beautiful… Ed Sheeran                 67 2yiy9cd2QktrN…\n 7 2OAylPUDDfwRGfe0lYql… Never Rea… Katy Perry                 62 7INHYSeusaFly…\n 8 6b1RNvAcJjQH73eZO4BL… Post Malo… Sam Feldt                  69 6703SRPsLkS4b…\n 9 7bF6tCO3gFb8INrEDcjN… Tough Lov… Avicii                     68 7CvAfGvq4RlIw…\n10 1IXGILkPm0tOCNeq00kC… If I Can'… Shawn Mendes               67 4QxzbfSsVryEQ…\n# ℹ 32,823 more rows\n# ℹ 18 more variables: track_album_name &lt;chr&gt;, track_album_release_date &lt;chr&gt;,\n#   playlist_name &lt;chr&gt;, playlist_id &lt;chr&gt;, playlist_genre &lt;chr&gt;,\n#   playlist_subgenre &lt;chr&gt;, danceability &lt;dbl&gt;, energy &lt;dbl&gt;, key &lt;dbl&gt;,\n#   loudness &lt;dbl&gt;, mode &lt;dbl&gt;, speechiness &lt;dbl&gt;, acousticness &lt;dbl&gt;,\n#   instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, valence &lt;dbl&gt;, tempo &lt;dbl&gt;,\n#   duration_ms &lt;dbl&gt;\n\n\nWe can take a glimpse at the data\n\nglimpse(spotify_songs)\n\nRows: 32,833\nColumns: 23\n$ track_id                 &lt;chr&gt; \"6f807x0ima9a1j3VPbc7VN\", \"0r7CVbZTWZgbTCYdfa…\n$ track_name               &lt;chr&gt; \"I Don't Care (with Justin Bieber) - Loud Lux…\n$ track_artist             &lt;chr&gt; \"Ed Sheeran\", \"Maroon 5\", \"Zara Larsson\", \"Th…\n$ track_popularity         &lt;dbl&gt; 66, 67, 70, 60, 69, 67, 62, 69, 68, 67, 58, 6…\n$ track_album_id           &lt;chr&gt; \"2oCs0DGTsRO98Gh5ZSl2Cx\", \"63rPSO264uRjW1X5E6…\n$ track_album_name         &lt;chr&gt; \"I Don't Care (with Justin Bieber) [Loud Luxu…\n$ track_album_release_date &lt;chr&gt; \"2019-06-14\", \"2019-12-13\", \"2019-07-05\", \"20…\n$ playlist_name            &lt;chr&gt; \"Pop Remix\", \"Pop Remix\", \"Pop Remix\", \"Pop R…\n$ playlist_id              &lt;chr&gt; \"37i9dQZF1DXcZDD7cfEKhW\", \"37i9dQZF1DXcZDD7cf…\n$ playlist_genre           &lt;chr&gt; \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"po…\n$ playlist_subgenre        &lt;chr&gt; \"dance pop\", \"dance pop\", \"dance pop\", \"dance…\n$ danceability             &lt;dbl&gt; 0.748, 0.726, 0.675, 0.718, 0.650, 0.675, 0.4…\n$ energy                   &lt;dbl&gt; 0.916, 0.815, 0.931, 0.930, 0.833, 0.919, 0.8…\n$ key                      &lt;dbl&gt; 6, 11, 1, 7, 1, 8, 5, 4, 8, 2, 6, 8, 1, 5, 5,…\n$ loudness                 &lt;dbl&gt; -2.634, -4.969, -3.432, -3.778, -4.672, -5.38…\n$ mode                     &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, …\n$ speechiness              &lt;dbl&gt; 0.0583, 0.0373, 0.0742, 0.1020, 0.0359, 0.127…\n$ acousticness             &lt;dbl&gt; 0.10200, 0.07240, 0.07940, 0.02870, 0.08030, …\n$ instrumentalness         &lt;dbl&gt; 0.00e+00, 4.21e-03, 2.33e-05, 9.43e-06, 0.00e…\n$ liveness                 &lt;dbl&gt; 0.0653, 0.3570, 0.1100, 0.2040, 0.0833, 0.143…\n$ valence                  &lt;dbl&gt; 0.518, 0.693, 0.613, 0.277, 0.725, 0.585, 0.1…\n$ tempo                    &lt;dbl&gt; 122.036, 99.972, 124.008, 121.956, 123.976, 1…\n$ duration_ms              &lt;dbl&gt; 194754, 162600, 176616, 169093, 189052, 16304…\n\n\nFor all of the questions below, you can ignore the missing values in the dataset, so e.g. when taking averages, just remove the missing values before taking the average, if needed."
  },
  {
    "objectID": "exercises-03.html#tasks",
    "href": "exercises-03.html#tasks",
    "title": "4  Workshop",
    "section": "4.3 Tasks",
    "text": "4.3 Tasks\nUse functions from dplyr and ggplot2 to answer the following questions.\n\nHow many songs are in each genre?\n\n\n# Add your solution here\n\n\nWhat is average value of energy and acousticness in the latin genre in this dataset?\n\n\n# Add your solution here\n\n\nCalculate the average duration of song (in minutes) across all subgenres. Which subgenre has the longest song on average?\n\n\n# Add your solution here\n\n\nMake two boxplots side-by-side of the danceability of songs stratifying by whether a song has a fast or slow tempo. Define fast tempo as any song that has a tempo above its median value. On average, which songs are more danceable?\n\nHint: You may find the case_when() function useful in this part, which can be used to map values from one variable to different values in a new variable (when used in a mutate() call).\n\n## Generate some random numbers\ndat &lt;- tibble(x = rnorm(100))\nslice(dat, 1:3)\n\n# A tibble: 3 × 1\n        x\n    &lt;dbl&gt;\n1  0.230 \n2 -0.0754\n3 -0.536 \n\n## Create a new column that indicates whether the value of 'x' is positive or negative\ndat %&gt;%\n        mutate(is_positive = case_when(\n                x &gt;= 0 ~ \"Yes\",\n                x &lt; 0 ~ \"No\"\n        ))\n\n# A tibble: 100 × 2\n         x is_positive\n     &lt;dbl&gt; &lt;chr&gt;      \n 1  0.230  Yes        \n 2 -0.0754 No         \n 3 -0.536  No         \n 4 -3.36   No         \n 5  0.491  Yes        \n 6  1.43   Yes        \n 7 -0.0423 No         \n 8 -0.339  No         \n 9  0.201  Yes        \n10  0.192  Yes        \n# ℹ 90 more rows\n\n\n\n# Add your solution here"
  },
  {
    "objectID": "intro-to-bioc.html#learning-objectives",
    "href": "intro-to-bioc.html#learning-objectives",
    "title": "5  Introduction to Bioconductor",
    "section": "5.1 Learning objectives",
    "text": "5.1 Learning objectives\n\nDescribe what is the Bioconductor and how it’s different than CRAN\nDescribe the package types in Bioconductor\nRecognize and work with core Bioconductor objects including SummarizedExperiment\nBe able to perform a basic differential expression analysis with bulk RNA-seq\nIntroduce the core Bioconductor object called SingleCellExperiment."
  },
  {
    "objectID": "intro-to-bioc.html#a-brief-history-of-bioconductor",
    "href": "intro-to-bioc.html#a-brief-history-of-bioconductor",
    "title": "5  Introduction to Bioconductor",
    "section": "5.2 A brief history of Bioconductor",
    "text": "5.2 A brief history of Bioconductor\nThe Bioconductor project was started in the Fall of 2001, as an initiative for the collaborative creation of extensible software for computational biology and bioinformatics.\nThe goal of the project is to develop tools for the statistical analysis and comprehension of large datasets and technological artifacts in rigorously and robustly designed experiments. Beyond statistical analyses, the interpretation of statistical results is supported by packages providing biological context, visualization, and reproducibility.\nOver the years, software packages contributed to the Bioconductor project have reflected the evolution and emergence of several high-throughput technologies, from microarrays to single-cell genomics, through many variations of sequencing experiments (e.g., RNA-seq, ChIP-seq, DNA-seq), analyses (e.g., sequence variation, copy number variation, single nucleotide polymorphisms), and data modalities (e.g., flow cytometry, proteomics, microscropy and image analysis).\nThe Bioconductor project culminates at an annual conference in North America in the summer, while regional conferences offer great opportunities for networking in Europe, Asia, and North America.\nThe project is committed to promote a diverse and inclusive community, including a Code of Conduct enforced by a Code of Conduct committee.\n\n\n\nTimeline of major Bioconductor milestones alongside technological advancements.\n\n\nTimeline of major Bioconductor milestones alongside technological advancements. Above the timeline, the figure marks the first occurrence of major events. Within the timeline, the name of packages providing core infrastructure indicate the release date. Below the timeline, major technological advancements contextualise the evolution of the Bioconductor project over time."
  },
  {
    "objectID": "intro-to-bioc.html#a-scientific-project",
    "href": "intro-to-bioc.html#a-scientific-project",
    "title": "5  Introduction to Bioconductor",
    "section": "5.3 A scientific project",
    "text": "5.3 A scientific project\n\nThe original publication describes the aims and methods of the project at its inception is Gentleman et al. (2004).\nHuber et al. (2015) illustrates the progression of the project, including descriptions of core infrastructure and case studies, from the perspective of both users and developers.\nAmezquita et al. (2020) reviewed further developments of the project in the wake of single-cell genomics technologies.\n\nMany more publications and book chapters cite the Bioconductor project, with recent example listed on the Bioconductor website.\nIn addition, there is a core team which is supported by an NIH grant, and developers who contribute to the open source Bioconductor packages."
  },
  {
    "objectID": "intro-to-bioc.html#a-package-repository",
    "href": "intro-to-bioc.html#a-package-repository",
    "title": "5  Introduction to Bioconductor",
    "section": "5.4 A package repository",
    "text": "5.4 A package repository"
  },
  {
    "objectID": "intro-to-bioc.html#overview-and-relationship-to-cran",
    "href": "intro-to-bioc.html#overview-and-relationship-to-cran",
    "title": "5  Introduction to Bioconductor",
    "section": "5.5 Overview and relationship to CRAN",
    "text": "5.5 Overview and relationship to CRAN\nUndoubtedly, software packages are the best-known aspect of the Bioconductor project. Since its inception in 2001, the repository has grown over time to host thousands of packages.\nThe Bioconductor project has extended the preexisting CRAN repository – much larger and general-purpose in scope – to comprise R packages primarily catering for bioinformatics and computational biology analyses."
  },
  {
    "objectID": "intro-to-bioc.html#the-bioconductor-release-cycle",
    "href": "intro-to-bioc.html#the-bioconductor-release-cycle",
    "title": "5  Introduction to Bioconductor",
    "section": "5.6 The Bioconductor release cycle",
    "text": "5.6 The Bioconductor release cycle\nThe Bioconductor project also extended the packaging infrastructure of the CRAN repository to better support the deployment and management of packages at the user level.\nIn particular, the Bioconductor projects features a 6-month release cycle (typically around April and October), which sees a snapshot of the current version of all packages in the Bioconductor repository earmarked for a specific version of R.\nR itself is released on an annual basis (typically around April), meaning that for each release of R, two compatible releases of Bioconductor packages are available.\n\nAs such, Bioconductor package developers are required to always use the version of R that will be associated with the next release of the Bioconductor project.\nThis means using the development version of R between October and April, and the release version of R between April and October.\n\nWhy? The strict Bioconductor release cycle prevents users from installing temporally distant versions of packages that were very likely never tested together.\nThis practice reflects the development cycle of packages of both CRAN and Bioconductor, where contemporaneous packages are regularly tested by automated systems to ensure that the latest software updates in package dependencies do not break downstream packages, or prompts those package maintainers to update their own software as a consequence.\nPrior to each Bioconductor release, packages that do not pass the requires suites of automated tests are deprecated and subsequently removed from the repository.\nThis ensures that each Bioconductor release provides a suite of packages that are mutually compatible, traceable, and guaranteed to function for the associated version of R.\n\n\n\nTimeline of release dates for selected Bioconductor and R versions.\n\n\nTimeline of release dates for selected Bioconductor and R versions. The upper section of the timeline indicates versions and approximate release dates for the R project. The lower section of the timeline indicates versions and release dates for the Bioconductor project. Source: Bioconductor."
  },
  {
    "objectID": "intro-to-bioc.html#package-types",
    "href": "intro-to-bioc.html#package-types",
    "title": "5  Introduction to Bioconductor",
    "section": "5.7 Package types",
    "text": "5.7 Package types\nPackages are broadly divided in four major categories:\n\nsoftware\nannotation data\nexperiment data\nworkflows\n\nSoftware packages themselves can be subdivided into packages that\n\nprovide infrastructure (i.e., classes) to store and access data\npackages that provide methodological tools to process data stored in those data structures\n\nThis separation of structure and analysis is at the core of the Bioconductor project, encouraging developers of new methodological software packages to thoughtfully re-use existing data containers where possible, and reducing the cognitive burden imposed on users who can more easily experiment with alternative workflows without the need to learn and convert between different data structures.\nAnnotation data packages provide self-contained databases of diverse genomic annotations (e.g., gene identifiers, biological pathways).\n\nDifferent collections of annotation packages can be found in the Bioconductor project.\nThey are identifiable by their respective naming pattern, and the information that they contain.\n\nFor instance, the so-called OrgDb packages (e.g., the org.Hs.eg.db package) provide information mapping different types of gene identifiers and pathway databases; the so-called EnsDb (e.g., EnsDb.Hsapiens.v86) packages encapsulate individual versions of the Ensembl annotations in Bioconductor packages; and the so-called TxDb packages (e.g., TxDb.Hsapiens.UCSC.hg38.knownGene) encapsulate individual versions UCSC gene annotation tables.\nExperiment data packages provide self-contained datasets that are often used by software package developers to demonstrate the use of their package on well-known standard datasets in their package vignettes.\nFinally, workflow packages exclusively provide collections of vignettes that demonstrate the combined usage of several other packages as a coherent workflow, but do not provide any new source code or functionality themselves."
  },
  {
    "objectID": "intro-to-bioc.html#online-communication-channels",
    "href": "intro-to-bioc.html#online-communication-channels",
    "title": "5  Introduction to Bioconductor",
    "section": "5.8 Online communication channels",
    "text": "5.8 Online communication channels\n\n5.8.1 Support site\nThe Bioconductor support site provides a platform where users and developers can communicate freely (following the Bioconductor Code of Conduct) to discuss issues on a range of subjects, ranging from packages to conceptual questions about best practices.\n\n\n5.8.2 Slack workspace\nThe Bioconductor Slack workspace is an open space that all community members are welcome to join (for free) and use for rapid interactions. Currently, the “Pro” pricing plan kindly supported by core funding provides:\n\nUnlimited message archive\nUnlimited apps\nGroup video calls with screen sharing\nWork securely with other organizations using Slack Connect\n\nA wide range of channels have been created to discuss a range of subjects, and community members can freely join the discussion on those channels of create new ones to discuss new subjects.\nImportant announcements are posted on the #general channel.\n\n\n5.8.3 Developer Mailing List\nThe bioc-devel@r-project.org mailing list is used for communication between package developers, and announcements from the Biocondutor core team.\n\n\n5.8.4 A scientific and technical community\n\nScientific Advisory Board (SAB) Meet Annually, External and Internal leader in the field who act as project advisors. No Term limits.\nTechnical Advisory Board (TAB). Meet monthly to consider technical aspects of core infastructure and scientific direction of the project. 15 members, 3 year term. Annual open-to-all elections to rotate members. Current officers are Vince Carey (chair), Levi Waldron (vice Chair) Charlotte Soneson (Secretary).\nCommunity Advisory Board (CAB) Meet monthly to consider community outreach, events, education and training. 15 members, 3 year term. Annual open-to-all elections to rotate members. Current officers are Aedin Culhane (chair), Matt Ritchie (co Chair), Lori Kern (Secretary).\nCode of Conduct committee"
  },
  {
    "objectID": "intro-to-bioc.html#getting-started-with-bioconductor",
    "href": "intro-to-bioc.html#getting-started-with-bioconductor",
    "title": "5  Introduction to Bioconductor",
    "section": "5.9 Getting started with Bioconductor",
    "text": "5.9 Getting started with Bioconductor\nBefore we get started, you need to know how to install Bioconductor packages. The most important details are:\n\nBioconductor is a package repository, like CRAN\nAll Bioconductor packages must be installed following the instructions here: https://bioconductor.org/install\nBioconductor packages are linked in their versions, both to each other and to the version of R\nBioconductor’s installation function will look up your version of R and give you the appropriate versions of Bioconductor packages\nIf you want the latest version of Bioconductor, you need to use the latest version of R.\n\nHow do you know if a package is a Bioconductor package?\nFor one thing, you can just google the package name and you will see either CRAN or Bioconductor as a first result (packages must be in one or the other, they are not allowed to be on both repositories).\nBut also, you can use Bioconductor’s installation function to install any packages, even ones on CRAN.\nBy the way, you can install multiple packages at once by making a string vector: BiocManager::install(c(\"foo\",\"bar\"))\n\n5.9.1 Working with Bioconductor objects\nBioconductor’s infrastructure is built up of object classes.\nAn example of a class is GRanges (stands for “genomic ranges”), which is a way to specify a set of ranges in a particular genome, e.g. from basepair 101 to basepair 200 on chromosome 1 of the human genome (version 38).\nWhat’s an object?\nWell everything in R is an object, but usually when we talk about Bioconductor objects, we mean data structures containing many attributes, so more complex than a vector or matrix.\nAnd the objects have specific methods that help you either access the information in the object, run analyses on the object, plot the object, etc.\nBioconductor also allows for class hierarchy, which means that you can define a class of object that inherits the structure and methods of a superclass on which it depends. This last point is mostly important for people who are developing new software for Bioconductor (maybe that’s you!)\nWe will introduce the core Bioconductor objects here.\n\n\n5.9.2 SummarizedExperiment (SE)\nFirst, we will discuss one of the most important classes of object, which is the SummarizedExperiment, or SE.\nSEs have the structure:\n\na matrix of data, rows are genomic features, and columns are samples\na table of data about the samples (columns)\na table of data about the features (rows)\n\nA diagram of this 3-part structure can be found here.\nIn SE, the 3 parts of the object are called 1) assay, 2) colData and 3) rowData or rowRanges.\nNote: There was a class of object that came before the SE, called the ExpressionSet, which was used primarily to store microarray data.\nHere we will skip over the ExpressionSet, and just look at SEs.\nIt helps to start by making a small toy SE, to see how the pieces come together. (Often you won’t make an SE manually, but it will be downloaded from an external source, or generated by a function that you call, e.g. the tximeta or some other data loading function.)\n\nlibrary(SummarizedExperiment)\ncol_data &lt;- data.frame(sample=factor(1:6),\n                       condition=factor(c(\"A\",\"A\",\"B\",\"B\",\"C\",\"C\")),\n                       treated=factor(rep(0:1,3)))\ncol_data\n\n  sample condition treated\n1      1         A       0\n2      2         A       1\n3      3         B       0\n4      4         B       1\n5      5         C       0\n6      6         C       1\n\n\nAn important aspect of SEs is that the rows can optionally correspond to particular set of GRanges\n\ne.g. a row of an SE could give the number of RNA-seq reads that can be assigned to a particular gene, and the row could also have metadata in the 3rd slot including, e.g. location of the gene in the genome.\n\nIn this case, we use the rowRanges slot to specify the information.\nIf we don’t have ranges, we can just put a table on the “side” of the SE by specifying rowData.\nI will show in the example though how to provide rowRanges.\nLet’s use the first 10 genes in the Ensembl database for human.\nThe following code loads a database, pulls out all the genes (as GRanges), removes extra “non-standard” chromosomes, and then subsets to the first 10 genes.\n\nlibrary(EnsDb.Hsapiens.v86)\ntxdb &lt;- EnsDb.Hsapiens.v86\ng &lt;- genes(txdb)\ng &lt;- keepStandardChromosomes(g, pruning.mode=\"coarse\")\nrow_ranges &lt;- g[1:10]\n\nWe will make up some simulated “expression” measurements, and then store these in the SE.\nI call list so I can name the matrix, otherwise it would not be named.\n\nexprs &lt;- matrix(rnorm(6 * 10), ncol=6, nrow=10)\nse &lt;- SummarizedExperiment(assay = list(\"exprs\" = exprs),\n                           colData = col_data,\n                           rowRanges = row_ranges)\nse\n\nclass: RangedSummarizedExperiment \ndim: 10 6 \nmetadata(0):\nassays(1): exprs\nrownames(10): ENSG00000223972 ENSG00000227232 ... ENSG00000238009\n  ENSG00000239945\nrowData names(6): gene_id gene_name ... symbol entrezid\ncolnames: NULL\ncolData names(3): sample condition treated\n\n\nWe see this object has one named matrix. The object could have multiple matrices (as long as these are the same shape).\nIn that case you could access the first with assay and in general by name, e.g. assay(se, \"exprs\") or equivalently assays(se)[[\"exprs\"]] .\n\nassayNames(se)\n\n[1] \"exprs\"\n\n\nFinally, if we wanted to add data onto the rows, for example, the score of a test on the matrix data, we use the metadata columns function, or mcols:\n\nmcols(se)$score &lt;- rnorm(10)\nmcols(se)\n\nDataFrame with 10 rows and 7 columns\n                        gene_id    gene_name           gene_biotype\n                    &lt;character&gt;  &lt;character&gt;            &lt;character&gt;\nENSG00000223972 ENSG00000223972      DDX11L1 transcribed_unproces..\nENSG00000227232 ENSG00000227232       WASH7P unprocessed_pseudogene\nENSG00000278267 ENSG00000278267    MIR6859-1                  miRNA\nENSG00000243485 ENSG00000243485    MIR1302-2                lincRNA\nENSG00000237613 ENSG00000237613      FAM138A                lincRNA\nENSG00000268020 ENSG00000268020       OR4G4P unprocessed_pseudogene\nENSG00000240361 ENSG00000240361      OR4G11P unprocessed_pseudogene\nENSG00000186092 ENSG00000186092        OR4F5         protein_coding\nENSG00000238009 ENSG00000238009 RP11-34P13.7                lincRNA\nENSG00000239945 ENSG00000239945 RP11-34P13.8                lincRNA\n                seq_coord_system       symbol                       entrezid\n                     &lt;character&gt;  &lt;character&gt;                         &lt;list&gt;\nENSG00000223972       chromosome      DDX11L1 100287596,100287102,727856,...\nENSG00000227232       chromosome       WASH7P                             NA\nENSG00000278267       chromosome    MIR6859-1                      102466751\nENSG00000243485       chromosome    MIR1302-2            105376912,100302278\nENSG00000237613       chromosome      FAM138A           654835,645520,641702\nENSG00000268020       chromosome       OR4G4P                             NA\nENSG00000240361       chromosome      OR4G11P                             NA\nENSG00000186092       chromosome        OR4F5                          79501\nENSG00000238009       chromosome RP11-34P13.7                             NA\nENSG00000239945       chromosome RP11-34P13.8                             NA\n                    score\n                &lt;numeric&gt;\nENSG00000223972  0.919522\nENSG00000227232  1.648822\nENSG00000278267 -0.941614\nENSG00000243485 -0.990802\nENSG00000237613 -0.446503\nENSG00000268020  0.678977\nENSG00000240361 -1.015432\nENSG00000186092 -0.867000\nENSG00000238009 -0.300285\nENSG00000239945 -0.584982\n\n\nAdding data to the column metadata is even easier, we can just use $:\n\nse$librarySize &lt;- runif(6,1e6,2e6)\ncolData(se)\n\nDataFrame with 6 rows and 4 columns\n    sample condition  treated librarySize\n  &lt;factor&gt;  &lt;factor&gt; &lt;factor&gt;   &lt;numeric&gt;\n1        1         A        0     1960730\n2        2         A        1     1741090\n3        3         B        0     1006700\n4        4         B        1     1808028\n5        5         C        0     1811178\n6        6         C        1     1901476\n\n\n\n\n5.9.3 Using the ranges of a SE\nHow does this additional functionality of the rowRanges facilitate faster data analysis?\nSuppose we are working with another data set besides se and we find a region of interest on chromsome 1.\nIf we want to pull out the expression data for that region, we just ask for the subset of se that overlaps.\nFirst, we build the query region, and then use the GRanges function overlapsAny() within single square brackets (like you would subset any matrix-like object):\n\nquery &lt;- GRanges(\"1\", IRanges(25000,40000))\nse_sub &lt;- se[overlapsAny(se, query), ]\n\nWe could have equivalently used the shorthand code:\n\nse_sub &lt;- se[se %over% query,]\n\nWe get just three ranges, and three rows of the SE:\n\nrowRanges(se_sub)\n\nGRanges object with 3 ranges and 7 metadata columns:\n                  seqnames      ranges strand |         gene_id   gene_name\n                     &lt;Rle&gt;   &lt;IRanges&gt;  &lt;Rle&gt; |     &lt;character&gt; &lt;character&gt;\n  ENSG00000227232        1 14404-29570      - | ENSG00000227232      WASH7P\n  ENSG00000243485        1 29554-31109      + | ENSG00000243485   MIR1302-2\n  ENSG00000237613        1 34554-36081      - | ENSG00000237613     FAM138A\n                            gene_biotype seq_coord_system      symbol\n                             &lt;character&gt;      &lt;character&gt; &lt;character&gt;\n  ENSG00000227232 unprocessed_pseudogene       chromosome      WASH7P\n  ENSG00000243485                lincRNA       chromosome   MIR1302-2\n  ENSG00000237613                lincRNA       chromosome     FAM138A\n                              entrezid     score\n                                &lt;list&gt; &lt;numeric&gt;\n  ENSG00000227232                 &lt;NA&gt;  1.648822\n  ENSG00000243485  105376912,100302278 -0.990802\n  ENSG00000237613 654835,645520,641702 -0.446503\n  -------\n  seqinfo: 25 sequences (1 circular) from GRCh38 genome\n\nassay(se_sub)\n\n                      [,1]       [,2]       [,3]       [,4]       [,5]\nENSG00000227232  0.4620206 -0.2642701  2.9981278 -1.5187187  0.5381596\nENSG00000243485  1.0155080 -1.8298211  0.9321233  0.3247443  0.5470970\nENSG00000237613 -0.2766870 -0.8681559 -0.2705575  1.5609302 -0.2008953\n                      [,6]\nENSG00000227232 -0.4973110\nENSG00000243485  0.6077315\nENSG00000237613 -1.0156904\n\n\nAnother useful property is that we know metadata about the chromosomes, and the version of the genome.\n\nNote: If you were not yet aware, the basepair position of a given feature, say gene XYZ, will change between versions of the genome, as sequences are added or rearranged.\n\n\nseqinfo(se)\n\nSeqinfo object with 25 sequences (1 circular) from GRCh38 genome:\n  seqnames seqlengths isCircular genome\n  1         248956422      FALSE GRCh38\n  10        133797422      FALSE GRCh38\n  11        135086622      FALSE GRCh38\n  12        133275309      FALSE GRCh38\n  13        114364328      FALSE GRCh38\n  ...             ...        ...    ...\n  8         145138636      FALSE GRCh38\n  9         138394717      FALSE GRCh38\n  MT            16569       TRUE GRCh38\n  X         156040895      FALSE GRCh38\n  Y          57227415      FALSE GRCh38\n\n\n\n\n5.9.4 Downloading SE data\nLet’s download a SE object from recount2, which performs a basic summarization of public data sets with gene expression data.\nThis dataset contains RNA-seq samples from human airway epithelial cell cultures.\nThe paper is here. The structure of the experiment was that, cell cultures from 6 asthmatic and 6 non-asthmatics donors were treated with viral infection or left untreated (controls).\nSo we have 2 samples (control or treated) for each of the 12 donors.\n\nlibrary(here)\n\nhere() starts at /Users/stephaniehicks/Documents/github/teaching/cshlcg2023\n\n# tests if a directory named \"data\" exists locally\nif(!dir.exists(here(\"data\"))) { dir.create(here(\"data\")) }\n\nfile &lt;- \"asthma.rda\"\nif (!file.exists(here(\"data\", file))){\n  url &lt;- \"http://duffel.rail.bio/recount/SRP046226/rse_gene.Rdata\"\n  download.file(url, here(\"data\", file))\n}\nload(here(\"data\", file))\n\nWe use a custom function to produce a matrix which a count of RNA fragments for each gene (rows) and each sample (columns).\n(Recount project calls these objects rse for RangedSummarizedExperiment, meaning it has rowRanges information.)\n\nmy_scale_counts &lt;- function(rse_gene, round=TRUE) {\n  cts &lt;- assays(rse_gene)$counts\n  # mapped_read_count includes the count for both reads of a pair\n  paired &lt;- ifelse(colData(rse_gene)$paired_end, 2, 1)\n  x &lt;- (colData(rse_gene)$mapped_read_count / paired) / colSums(cts)\n  assays(rse_gene)$counts &lt;- t(t(assays(rse_gene)$counts) * x)\n  if (round) {\n    assays(rse_gene)$counts &lt;- round(assays(rse_gene)$counts)\n  }\n  rse_gene\n}\n\nrse &lt;- my_scale_counts(rse_gene)\n\nWe can take a peek at the column data:\n\ncolData(rse)[,1:6]\n\nDataFrame with 24 rows and 6 columns\n               project      sample  experiment         run\n           &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt;\nSRR1565926   SRP046226   SRS694613   SRX692912  SRR1565926\nSRR1565927   SRP046226   SRS694614   SRX692913  SRR1565927\nSRR1565928   SRP046226   SRS694615   SRX692914  SRR1565928\nSRR1565929   SRP046226   SRS694616   SRX692915  SRR1565929\nSRR1565930   SRP046226   SRS694617   SRX692916  SRR1565930\n...                ...         ...         ...         ...\nSRR1565945   SRP046226   SRS694632   SRX692931  SRR1565945\nSRR1565946   SRP046226   SRS694633   SRX692932  SRR1565946\nSRR1565947   SRP046226   SRS694634   SRX692933  SRR1565947\nSRR1565948   SRP046226   SRS694635   SRX692934  SRR1565948\nSRR1565949   SRP046226   SRS694636   SRX692935  SRR1565949\n           read_count_as_reported_by_sra reads_downloaded\n                               &lt;integer&gt;        &lt;integer&gt;\nSRR1565926                      12866750         12866750\nSRR1565927                      12797108         12797108\nSRR1565928                      13319016         13319016\nSRR1565929                      13725752         13725752\nSRR1565930                      10882416         10882416\n...                                  ...              ...\nSRR1565945                      13791854         13791854\nSRR1565946                      13480842         13480842\nSRR1565947                      13166594         13166594\nSRR1565948                      13320398         13320398\nSRR1565949                      13002276         13002276\n\n\nThe information we are interested in is contained in the characteristics column (which is a character list).\n\nclass(rse$characteristics)\n\n[1] \"CompressedCharacterList\"\nattr(,\"package\")\n[1] \"IRanges\"\n\nrse$characteristics[1:3]\n\nCharacterList of length 3\n[[1]] cell type: Isolated from human trachea-bronchial tissues ...\n[[2]] cell type: Isolated from human trachea-bronchial tissues ...\n[[3]] cell type: Isolated from human trachea-bronchial tissues ...\n\nrse$characteristics[[1]]\n\n[1] \"cell type: Isolated from human trachea-bronchial tissues\"\n[2] \"passages: 2\"                                             \n[3] \"disease state: asthmatic\"                                \n[4] \"treatment: HRV16\"                                        \n\n\nWe can pull out the 3 and 4 element using the sapply function and the square bracket function.\nI know this syntax looks a little funny, but it’s really just saying, use the single square bracket, pull out the third element (or fourth element).\n\nrse$condition &lt;- sapply(rse$characteristics, `[`, 3)\nrse$treatment &lt;- sapply(rse$characteristics, `[`, 4)\ntable(rse$condition, rse$treatment)\n\n                              \n                               treatment: HRV16 treatment: Vehicle\n  disease state: asthmatic                    6                  6\n  disease state: non-asthmatic                6                  6\n\n\nLet’s see what the rowRanges of this experiment look like:\n\nrowRanges(rse)\n\nGRanges object with 58037 ranges and 3 metadata columns:\n                     seqnames              ranges strand |            gene_id\n                        &lt;Rle&gt;           &lt;IRanges&gt;  &lt;Rle&gt; |        &lt;character&gt;\n  ENSG00000000003.14     chrX 100627109-100639991      - | ENSG00000000003.14\n   ENSG00000000005.5     chrX 100584802-100599885      + |  ENSG00000000005.5\n  ENSG00000000419.12    chr20   50934867-50958555      - | ENSG00000000419.12\n  ENSG00000000457.13     chr1 169849631-169894267      - | ENSG00000000457.13\n  ENSG00000000460.16     chr1 169662007-169854080      + | ENSG00000000460.16\n                 ...      ...                 ...    ... .                ...\n   ENSG00000283695.1    chr19   52865369-52865429      - |  ENSG00000283695.1\n   ENSG00000283696.1     chr1 161399409-161422424      + |  ENSG00000283696.1\n   ENSG00000283697.1     chrX 149548210-149549852      - |  ENSG00000283697.1\n   ENSG00000283698.1     chr2 112439312-112469687      - |  ENSG00000283698.1\n   ENSG00000283699.1    chr10   12653138-12653197      - |  ENSG00000283699.1\n                     bp_length          symbol\n                     &lt;integer&gt; &lt;CharacterList&gt;\n  ENSG00000000003.14      4535          TSPAN6\n   ENSG00000000005.5      1610            TNMD\n  ENSG00000000419.12      1207            DPM1\n  ENSG00000000457.13      6883           SCYL3\n  ENSG00000000460.16      5967        C1orf112\n                 ...       ...             ...\n   ENSG00000283695.1        61            &lt;NA&gt;\n   ENSG00000283696.1       997            &lt;NA&gt;\n   ENSG00000283697.1      1184    LOC101928917\n   ENSG00000283698.1       940            &lt;NA&gt;\n   ENSG00000283699.1        60         MIR4481\n  -------\n  seqinfo: 25 sequences (1 circular) from an unspecified genome; no seqlengths\n\nseqinfo(rse)\n\nSeqinfo object with 25 sequences (1 circular) from an unspecified genome; no seqlengths:\n  seqnames seqlengths isCircular genome\n  chr1           &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;\n  chr2           &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;\n  chr3           &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;\n  chr4           &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;\n  chr5           &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;\n  ...             ...        ...    ...\n  chr21          &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;\n  chr22          &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;\n  chrX           &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;\n  chrY           &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;\n  chrM           &lt;NA&gt;       TRUE   &lt;NA&gt;\n\n\nThe rowRanges here were determined by the quantification method that the recount2 authors used.\nWe don’t know what the genome is from the seqinfo, but we could look this up from the project website.\nThe following code I use to clean up the condition and treatment variables:\n\nlibrary(magrittr)\nrse$condition %&lt;&gt;% (function(x) {\n  factor(sub(\"-\",\".\", sub(\"disease state: (.*)\",\"\\\\1\",x) ))\n  })\nrse$treatment %&lt;&gt;% (function(x) factor(sub(\"treatment: (.*)\",\"\\\\1\",x)))\n\nNow we have:\n\ntable(rse$condition, rse$treatment)\n\n               \n                HRV16 Vehicle\n  asthmatic         6       6\n  non.asthmatic     6       6\n\n\n\n\n5.9.5 Visualizing count matrix data in a SE\nHere we just use a transformation so that we can compute meaningful distances on count data (without a larger discussion on normalization).\nWe build a DESeqDataSet and then specify the experimental design using a ~ and the variables that we expect to produce differences in the counts. (These variables are used to assess how much technical variability is in the data, but not used in the transformation function itself.)\n\nlibrary(DESeq2)\ndds &lt;- DESeqDataSet(rse, ~condition + treatment)\n\nconverting counts to integer mode\n\n\nWe use this function, which implements a variance stabilizing transformation:\n\nvsd &lt;- vst(dds)\n\nWe calculate the variance across all samples (on the transformed data):\n\nlibrary(matrixStats)\nrv &lt;- rowVars(assay(vsd))\no &lt;- order(rv, decreasing=TRUE)[1:100]\n\nFinally, before plotting a heatmap, we extract the covariates that we want to annotated the top of the plot.\n\nanno_col &lt;- as.data.frame(colData(vsd)[,c(\"condition\",\"treatment\")])\nanno_col\n\n               condition treatment\nSRR1565926     asthmatic     HRV16\nSRR1565927     asthmatic     HRV16\nSRR1565928     asthmatic     HRV16\nSRR1565929     asthmatic     HRV16\nSRR1565930     asthmatic     HRV16\nSRR1565931     asthmatic     HRV16\nSRR1565932     asthmatic   Vehicle\nSRR1565933     asthmatic   Vehicle\nSRR1565934     asthmatic   Vehicle\nSRR1565935     asthmatic   Vehicle\nSRR1565936     asthmatic   Vehicle\nSRR1565937     asthmatic   Vehicle\nSRR1565938 non.asthmatic     HRV16\nSRR1565939 non.asthmatic     HRV16\nSRR1565940 non.asthmatic     HRV16\nSRR1565941 non.asthmatic     HRV16\nSRR1565942 non.asthmatic     HRV16\nSRR1565943 non.asthmatic     HRV16\nSRR1565944 non.asthmatic   Vehicle\nSRR1565945 non.asthmatic   Vehicle\nSRR1565946 non.asthmatic   Vehicle\nSRR1565947 non.asthmatic   Vehicle\nSRR1565948 non.asthmatic   Vehicle\nSRR1565949 non.asthmatic   Vehicle\n\n\nThis code pull out the top of the transformed data by variance, and adds an annotation to the top of the plot.\nBy default the rows and columns will be clustered by Euclidean distance. See ?pheatmap for more details on this function (it’s a very detailed manual page).\n\nlibrary(pheatmap)\npheatmap(assay(vsd)[o,],\n         annotation_col=anno_col,\n         show_rownames=FALSE, \n         show_colnames=FALSE)\n\n\n\n\nWe can also easily make a PCA plot with dedicated functions:\n\nplotPCA(vsd, intgroup=\"treatment\")"
  },
  {
    "objectID": "intro-to-bioc.html#singlecellexperiment",
    "href": "intro-to-bioc.html#singlecellexperiment",
    "title": "5  Introduction to Bioconductor",
    "section": "5.10 SingleCellExperiment",
    "text": "5.10 SingleCellExperiment\nAn example of a class that extends the SE is SingleCellExperiment. This is a special object type for looking at single cell data.\nFor more details, there is a free online book “Orchestrating Single Cell Analysis With Bioconductor” produced by a group within the Bioconductor Project, with lots of example analyses: OSCA.\nHere we show a quick example of how this object extends the SE.\n\nlibrary(SingleCellExperiment)\nsce &lt;- as(rse, \"SingleCellExperiment\")\nsce\n\nclass: SingleCellExperiment \ndim: 58037 24 \nmetadata(0):\nassays(1): counts\nrownames(58037): ENSG00000000003.14 ENSG00000000005.5 ...\n  ENSG00000283698.1 ENSG00000283699.1\nrowData names(3): gene_id bp_length symbol\ncolnames(24): SRR1565926 SRR1565927 ... SRR1565948 SRR1565949\ncolData names(23): project sample ... condition treatment\nreducedDimNames(0):\nmainExpName: NULL\naltExpNames(0):\n\n\nThere are special functions dedicated to scaling the samples (we will discuss this technical aspect soon):\n\nlibrary(scran)\n\nLoading required package: scuttle\n\nsce &lt;- computeSumFactors(sce)\nsizeFactors(sce)\n\n [1] 0.7672143 0.8205514 0.8686567 0.9479224 0.6484723 0.9815079 1.0797070\n [8] 1.0569889 1.4377886 0.9465292 1.4759422 1.2630195 0.8889808 1.0524670\n[15] 0.9677885 0.8086102 0.8806503 0.8999780 0.9505805 1.0430322 1.2527967\n[22] 0.9908707 0.5208294 1.4491155\n\n\nSimilarly, dedicated functions for transformations:\n\nsce &lt;- logNormCounts(sce)\nassayNames(sce)\n\n[1] \"counts\"    \"logcounts\"\n\n\nAnd dedicated functions and new slots for reduced dimensions:\n\nset.seed(1)\nsce &lt;- fixedPCA(sce, rank=5, subset.row=NULL)\nreducedDimNames(sce)\n\n[1] \"PCA\"\n\n\nWe can manually get at the PCs:\n\npca &lt;- reducedDim(sce, \"PCA\")\nplot(pca[,1:2])\n\n\n\n\nBut we can more easily use dedicated visualization functions:\n\nlibrary(scater)\nplotReducedDim(sce, \"PCA\", colour_by=\"treatment\")"
  },
  {
    "objectID": "intro-to-bioc.html#session-info",
    "href": "intro-to-bioc.html#session-info",
    "title": "5  Introduction to Bioconductor",
    "section": "5.11 Session Info",
    "text": "5.11 Session Info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] scater_1.28.0               ggplot2_3.4.4              \n [3] scran_1.28.2                scuttle_1.9.4              \n [5] SingleCellExperiment_1.22.0 pheatmap_1.0.12            \n [7] DESeq2_1.40.2               magrittr_2.0.3             \n [9] here_1.0.1                  EnsDb.Hsapiens.v86_2.99.0  \n[11] ensembldb_2.24.1            AnnotationFilter_1.24.0    \n[13] GenomicFeatures_1.52.2      AnnotationDbi_1.62.2       \n[15] SummarizedExperiment_1.30.2 Biobase_2.60.0             \n[17] GenomicRanges_1.52.1        GenomeInfoDb_1.36.4        \n[19] IRanges_2.34.1              S4Vectors_0.38.2           \n[21] BiocGenerics_0.46.0         MatrixGenerics_1.12.3      \n[23] matrixStats_1.0.0          \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3        rstudioapi_0.15.0        \n  [3] jsonlite_1.8.7            ggbeeswarm_0.7.2         \n  [5] farver_2.1.1              rmarkdown_2.25           \n  [7] BiocIO_1.10.0             zlibbioc_1.46.0          \n  [9] vctrs_0.6.4               memoise_2.0.1            \n [11] Rsamtools_2.16.0          DelayedMatrixStats_1.22.6\n [13] RCurl_1.98-1.12           htmltools_0.5.6.1        \n [15] S4Arrays_1.0.6            progress_1.2.2           \n [17] curl_5.1.0                BiocNeighbors_1.18.0     \n [19] htmlwidgets_1.6.2         cachem_1.0.8             \n [21] GenomicAlignments_1.36.0  igraph_1.5.1             \n [23] lifecycle_1.0.3           pkgconfig_2.0.3          \n [25] rsvd_1.0.5                Matrix_1.6-1.1           \n [27] R6_2.5.1                  fastmap_1.1.1            \n [29] GenomeInfoDbData_1.2.10   digest_0.6.33            \n [31] colorspace_2.1-0          rprojroot_2.0.3          \n [33] dqrng_0.3.1               irlba_2.3.5.1            \n [35] RSQLite_2.3.1             beachmat_2.16.0          \n [37] filelock_1.0.2            labeling_0.4.3           \n [39] fansi_1.0.5               httr_1.4.7               \n [41] abind_1.4-5               compiler_4.3.1           \n [43] bit64_4.0.5               withr_2.5.2              \n [45] BiocParallel_1.34.2       viridis_0.6.4            \n [47] DBI_1.1.3                 biomaRt_2.56.1           \n [49] rappdirs_0.3.3            DelayedArray_0.26.7      \n [51] rjson_0.2.21              bluster_1.10.0           \n [53] tools_4.3.1               vipor_0.4.5              \n [55] beeswarm_0.4.0            glue_1.6.2               \n [57] restfulr_0.0.15           grid_4.3.1               \n [59] cluster_2.1.4             generics_0.1.3           \n [61] gtable_0.3.4              hms_1.1.3                \n [63] BiocSingular_1.16.0       ScaledMatrix_1.8.1       \n [65] metapod_1.7.0             xml2_1.3.5               \n [67] utf8_1.2.4                XVector_0.40.0           \n [69] ggrepel_0.9.4             pillar_1.9.0             \n [71] stringr_1.5.0             limma_3.56.2             \n [73] dplyr_1.1.3               BiocFileCache_2.8.0      \n [75] lattice_0.22-5            rtracklayer_1.60.1       \n [77] bit_4.0.5                 tidyselect_1.2.0         \n [79] locfit_1.5-9.8            Biostrings_2.68.1        \n [81] knitr_1.44                gridExtra_2.3            \n [83] ProtGenerics_1.32.0       edgeR_3.42.4             \n [85] xfun_0.40                 statmod_1.5.0            \n [87] stringi_1.7.12            lazyeval_0.2.2           \n [89] yaml_2.3.7                evaluate_0.22            \n [91] codetools_0.2-19          tibble_3.2.1             \n [93] BiocManager_1.30.22       cli_3.6.1                \n [95] munsell_0.5.0             Rcpp_1.0.11              \n [97] dbplyr_2.3.4              png_0.1-8                \n [99] XML_3.99-0.15             parallel_4.3.1           \n[101] blob_1.2.4                prettyunits_1.2.0        \n[103] sparseMatrixStats_1.12.2  bitops_1.0-7             \n[105] viridisLite_0.4.2         scales_1.2.1             \n[107] crayon_1.5.2              BiocStyle_2.28.1         \n[109] rlang_1.1.1               KEGGREST_1.40.1"
  },
  {
    "objectID": "intro-to-single-cell.html#part-1",
    "href": "intro-to-single-cell.html#part-1",
    "title": "6  Overview of single-cell analysis in R/Bioconductor",
    "section": "6.1 Part 1",
    "text": "6.1 Part 1\n\n6.1.1 Learning objectives\n\nUnderstand how count matrices are created from single-cell experimental platforms and protocols\nRecognize which basic principles and concepts were transferred from bulk to single-cell data analyses\nUnderstand the key differences between bulk and single-cell data\nDefine what is a “Unique Molecular Identifier”\nDefine multiplexing (and demultiplexing)\nOverview of basics in single-cell data analyses including quality control, normalization, feature selection, dimensionality reduction, and clustering\n\n\n\n6.1.2 Materials\nWe will go through the slides available here:\n\nhttps://docs.google.com/presentation/d/1XHfN-5NCXtu1P4reotrckGplPXJBTlQraFpd8ViPf1g/edit?usp=sharing"
  },
  {
    "objectID": "intro-to-single-cell.html#part-2",
    "href": "intro-to-single-cell.html#part-2",
    "title": "6  Overview of single-cell analysis in R/Bioconductor",
    "section": "6.2 Part 2",
    "text": "6.2 Part 2\n\n6.2.1 Learning objectives\n\nBe able to create a single-cell count matrix and read it into R\nRecognize and define the SingleCellExperiment S4 class in R/Bioconductor to store single-cell data\n\n\n\n6.2.2 Overview\nNGS data from scRNA-seq experiments must be converted into a matrix of expression values.\nThis is usually a count matrix containing the number of reads (or UMIs) mapped to each gene (row) in each cell (column). Once this quantification is complete, we can proceed with our downstream statistical analyses in R.\nConstructing a count matrix from raw scRNA-seq data requires some thought as the term “single-cell RNA-seq” encompasses a variety of different experimental protocols. This includes\n\ndroplet-based protocols like 10X Genomics, inDrop and Drop-seq\nplate-based protocols with UMIs like CEL-seq(2) and MARS-seq\nplate-based protocols with reads (mostly Smart-seq2)\nothers like sciRNA-seq, etc\n\nEach approach requires a different processing pipeline to deal with cell demultiplexing and UMI deduplication (if applicable). Here, we will briefly describe some of the methods used to generate a count matrix and read it into R.\n\n\n6.2.3 Creating a count matrix\nAs mentioned above, the exact procedure for quantifying expression depends on the technology involved:\n\nFor 10X Genomics data, the Cellranger software suite (Zheng et al. 2017) provides a custom pipeline to obtain a count matrix. This uses STAR to align reads to the reference genome and then counts the number of unique UMIs mapped to each gene.\nAlternatively, pseudo-alignment methods such as alevin (Srivastava et al. 2019) can be used to obtain a count matrix from the same data. This avoids the need for explicit alignment, which reduces the compute time and memory usage.\nFor other highly multiplexed protocols, the scPipe package provides a more general pipeline for processing scRNA-seq data. This uses the Rsubread aligner to align reads and then counts reads or UMIs per gene.\nFor CEL-seq or CEL-seq2 data, the scruff package provides a dedicated pipeline for quantification.\nFor read-based protocols, we can generally re-use the same pipelines for processing bulk RNA-seq data (e.g. Subread, RSEM, salmon)\nFor any data involving spike-in transcripts, the spike-in sequences should be included as part of the reference genome during alignment and quantification.\n\nIn all cases, the identity of the genes in the count matrix should be defined with standard identifiers from Ensembl or Entrez. These provide an unambiguous mapping between each row of the matrix and the corresponding gene.\nIn contrast, a single gene symbol may be used by multiple loci, or the mapping between symbols and genes may change over time, e.g., if the gene is renamed.\nThis makes it difficult to re-use the count matrix as we cannot be confident in the meaning of the symbols.\n(Of course, identifiers can be easily converted to gene symbols later on in the analysis. This is the recommended approach as it allows us to document how the conversion was performed and to backtrack to the stable identifiers if the symbols are ambiguous.)\n\n\n6.2.4 SingleCellExperiment Class\nOne of the main strengths of the Bioconductor project lies in the use of a common data infrastructure that powers interoperability across packages.\nUsers should be able to analyze their data using functions from different Bioconductor packages without the need to convert between formats.\nTo this end, the SingleCellExperiment class (from the SingleCellExperiment package) serves as the common currency for data exchange across 70+ single-cell-related Bioconductor packages.\nThis class implements a data structure that stores all aspects of our single-cell data - gene-by-cell expression data, per-cell metadata and per-gene annotation - and manipulate them in a synchronized manner.\n\n\n\n\n\n\n\n\n\n[Amezquita et al. 2019 (https://doi.org/10.1101/590562)]\n\nEach piece of (meta)data in the SingleCellExperiment is represented by a separate “slot”.\n\n(This terminology comes from the S4 class system, but that’s not important right now).\nIf we imagine the SingleCellExperiment object to be a cargo ship, the slots can be thought of as individual cargo boxes with different contents, e.g., certain slots expect numeric matrices whereas others may expect data frames.\nIf you want to know more about the available slots, their expected formats, and how we can interact with them, check out this chapter.\n\n\n\n\n\n\nSingleCellExperiment Example\n\n\n\nLet’s show you what a SingleCellExperiment (or sce for short) looks like.\n\nsce\n\nclass: SingleCellExperiment \ndim: 20006 3005 \nmetadata(0):\nassays(1): counts\nrownames(20006): Tspan12 Tshz1 ... mt-Rnr1 mt-Nd4l\nrowData names(1): featureType\ncolnames(3005): 1772071015_C02 1772071017_G12 ... 1772066098_A12\n  1772058148_F03\ncolData names(10): tissue group # ... level1class level2class\nreducedDimNames(0):\nmainExpName: endogenous\naltExpNames(2): ERCC repeat\n\n\nThis SingleCellExperiment object has 20006 genes and 3005 cells.\nWe can pull out the counts matrix with the counts() function and the corresponding rowData() and colData():\n\ncounts(sce)[1:5, 1:5]\n\n         1772071015_C02 1772071017_G12 1772071017_A05 1772071014_B06\nTspan12               0              0              0              3\nTshz1                 3              1              0              2\nFnbp1l                3              1              6              4\nAdamts15              0              0              0              0\nCldn12                1              1              1              0\n         1772067065_H06\nTspan12               0\nTshz1                 2\nFnbp1l                1\nAdamts15              0\nCldn12                0\n\nrowData(sce)\n\nDataFrame with 20006 rows and 1 column\n         featureType\n         &lt;character&gt;\nTspan12   endogenous\nTshz1     endogenous\nFnbp1l    endogenous\nAdamts15  endogenous\nCldn12    endogenous\n...              ...\nmt-Co2          mito\nmt-Co1          mito\nmt-Rnr2         mito\nmt-Rnr1         mito\nmt-Nd4l         mito\n\ncolData(sce)\n\nDataFrame with 3005 rows and 10 columns\n                       tissue   group # total mRNA mol      well       sex\n                  &lt;character&gt; &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\n1772071015_C02       sscortex         1           1221         3         3\n1772071017_G12       sscortex         1           1231        95         1\n1772071017_A05       sscortex         1           1652        27         1\n1772071014_B06       sscortex         1           1696        37         3\n1772067065_H06       sscortex         1           1219        43         3\n...                       ...       ...            ...       ...       ...\n1772067059_B04 ca1hippocampus         9           1997        19         1\n1772066097_D04 ca1hippocampus         9           1415        21         1\n1772063068_D01       sscortex         9           1876        34         3\n1772066098_A12 ca1hippocampus         9           1546        88         1\n1772058148_F03       sscortex         9           1970        15         3\n                     age  diameter        cell_id       level1class level2class\n               &lt;numeric&gt; &lt;numeric&gt;    &lt;character&gt;       &lt;character&gt; &lt;character&gt;\n1772071015_C02         2         1 1772071015_C02      interneurons       Int10\n1772071017_G12         1       353 1772071017_G12      interneurons       Int10\n1772071017_A05         1        13 1772071017_A05      interneurons        Int6\n1772071014_B06         2        19 1772071014_B06      interneurons       Int10\n1772067065_H06         6        12 1772067065_H06      interneurons        Int9\n...                  ...       ...            ...               ...         ...\n1772067059_B04         4       382 1772067059_B04 endothelial-mural       Peric\n1772066097_D04         7        12 1772066097_D04 endothelial-mural        Vsmc\n1772063068_D01         7       268 1772063068_D01 endothelial-mural        Vsmc\n1772066098_A12         7       324 1772066098_A12 endothelial-mural        Vsmc\n1772058148_F03         7         6 1772058148_F03 endothelial-mural        Vsmc"
  },
  {
    "objectID": "intro-to-single-cell.html#part-3",
    "href": "intro-to-single-cell.html#part-3",
    "title": "6  Overview of single-cell analysis in R/Bioconductor",
    "section": "6.3 Part 3",
    "text": "6.3 Part 3\n\n6.3.1 Learning objectives\n\nBe able to describe a standard workflow for analyzing single-cell data\nBe able to run code for a standard workflow starting from loading a SingleCellExperiment in R and identifying clusters.\n\n\n\n6.3.2 Overview\nHere, we provide an overview of the framework of a typical scRNA-seq analysis workflow:\n\n\n\n\n\n\n\n\n\nIn the simplest case, the workflow has the following form:\n\nWe compute quality control metrics to remove low-quality cells that would interfere with downstream analyses. These cells may have been damaged during processing or may not have been fully captured by the sequencing protocol. Common metrics includes the total counts per cell, the proportion of spike-in or mitochondrial reads and the number of detected features.\nWe convert the counts into normalized expression values to eliminate cell-specific biases (e.g., in capture efficiency). This allows us to perform explicit comparisons across cells in downstream steps like clustering. We also apply a transformation, typically log, to adjust for the mean-variance relationship.\nWe perform feature selection to pick a subset of interesting features for downstream analysis. This is done by modelling the variance across cells for each gene and retaining genes that are highly variable. The aim is to reduce computational overhead and noise from uninteresting genes.\nWe apply dimensionality reduction to compact the data and further reduce noise. Principal components analysis is typically used to obtain an initial low-rank representation for more computational work, followed by more aggressive methods like \\(t\\)-stochastic neighbor embedding for visualization purposes.\nWe cluster cells into groups according to similarities in their (normalized) expression profiles. This aims to obtain groupings that serve as empirical proxies for distinct biological states. We typically interpret these groupings by identifying differentially expressed marker genes between clusters.\n\n\n\n6.3.3 Quick start (simple)\nHere, we use the a droplet-based retina dataset from Macosko et al. (2015), provided in the scRNAseq package. This starts from a count matrix and finishes with clusters in preparation for biological interpretation. We also demonstrate how to identify differentially expressed genes between the clusters.\n\nlibrary(scRNAseq)\nsce &lt;- MacoskoRetinaData()\n\nsee ?scRNAseq and browseVignettes('scRNAseq') for documentation\n\n\nloading from cache\n\n\nsee ?scRNAseq and browseVignettes('scRNAseq') for documentation\n\n\nloading from cache\n\n# Quality control (using mitochondrial genes).\nlibrary(scater)\nis.mito &lt;- grepl(\"^MT-\", rownames(sce))\nqcstats &lt;- perCellQCMetrics(sce, subsets=list(Mito=is.mito))\nfiltered &lt;- quickPerCellQC(qcstats, percent_subsets=\"subsets_Mito_percent\")\nsce &lt;- sce[, !filtered$discard]\n\n# Normalization.\nsce &lt;- logNormCounts(sce)\n\n# Feature selection.\nlibrary(scran)\ndec &lt;- modelGeneVar(sce)\nhvg &lt;- getTopHVGs(dec, prop=0.1)\n\n# PCA.\nlibrary(scater)\nset.seed(1234)\nsce &lt;- runPCA(sce, ncomponents=25, subset_row=hvg)\n\n# Clustering.\nlibrary(bluster)\ncolLabels(sce) &lt;- clusterCells(sce, use.dimred='PCA',\n    BLUSPARAM=NNGraphParam(cluster.fun=\"louvain\"))    \n\n\n# Visualization.\nsce &lt;- runUMAP(sce, dimred = 'PCA')\nplotUMAP(sce, colour_by=\"label\")\n\n\n\n\nUMAP plot of the retina dataset, where each point is a cell and is colored by the assigned cluster identity.\n\n\n\n\n\n# Marker detection (between pairs of groups)\nmarkers &lt;- findMarkers(sce, test.type=\"wilcox\", direction=\"up\", lfc=1)\nlength(markers)\n\n[1] 13\n\nmarkers[[1]]\n\nDataFrame with 24658 rows and 16 columns\n                    Top      p.value          FDR summary.AUC     AUC.2\n              &lt;integer&gt;    &lt;numeric&gt;    &lt;numeric&gt;   &lt;numeric&gt; &lt;numeric&gt;\nMEG3                  1  0.00000e+00  0.00000e+00    0.867306  0.857315\nTUBA1A                1  3.58784e-81  5.89793e-78    0.609195  0.479862\nSNHG11                1  0.00000e+00  0.00000e+00    0.737419  0.747343\nSYT1                  2 3.51850e-268 1.08449e-264    0.786614  0.454677\nCALM1                 2  0.00000e+00  0.00000e+00    0.812019  0.665619\n...                 ...          ...          ...         ...       ...\nVSIG1             24654            1            1           0         0\nGM16390           24655            1            1           0         0\nGM25207           24656            1            1           0         0\n1110059M19RIK     24657            1            1           0         0\nGM20861           24658            1            1           0         0\n                  AUC.3     AUC.4     AUC.5     AUC.6     AUC.7     AUC.8\n              &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nMEG3          0.6814265  0.463403  0.806255  0.478814 0.1822596  0.888640\nTUBA1A        0.1162662  0.315768  0.422655  0.289249 0.4529957  0.541010\nSNHG11        0.0822847  0.562474  0.741261  0.703891 0.0228628  0.742098\nSYT1          0.5354274  0.317921  0.786614  0.307464 0.0591646  0.549534\nCALM1         0.1535742  0.370682  0.715928  0.425258 0.0105809  0.791806\n...                 ...       ...       ...       ...       ...       ...\nVSIG1                 0         0         0         0         0         0\nGM16390               0         0         0         0         0         0\nGM25207               0         0         0         0         0         0\n1110059M19RIK         0         0         0         0         0         0\nGM20861               0         0         0         0         0         0\n                  AUC.9     AUC.10    AUC.11    AUC.12    AUC.13\n              &lt;numeric&gt;  &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nMEG3           0.850705 0.48570145  0.866916  0.873796  0.867306\nTUBA1A         0.509171 0.33506617  0.552997  0.566389  0.609195\nSNHG11         0.745361 0.74450392  0.737413  0.744927  0.737419\nSYT1           0.823158 0.39904104  0.575948  0.584176  0.633476\nCALM1          0.444978 0.00587859  0.749949  0.834002  0.812019\n...                 ...        ...       ...       ...       ...\nVSIG1                 0          0         0         0         0\nGM16390               0          0         0         0         0\nGM25207               0          0         0         0         0\n1110059M19RIK         0          0         0         0         0\nGM20861               0          0         0         0         0\n\n\n\n\n6.3.4 Quick start (multiple batches)\nIf you could like to see Quick Start for scRNA-seq data with multiple batches, check out this workflow:\n\nhttps://bioconductor.org/books/3.15/OSCA.intro/analysis-overview.html#quick-start-multiple-batches"
  },
  {
    "objectID": "intro-to-single-cell.html#session-info",
    "href": "intro-to-single-cell.html#session-info",
    "title": "6  Overview of single-cell analysis in R/Bioconductor",
    "section": "6.4 Session Info",
    "text": "6.4 Session Info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] bluster_1.10.0              scran_1.28.2               \n [3] scRNAseq_2.14.0             scater_1.28.0              \n [5] ggplot2_3.4.4               scuttle_1.9.4              \n [7] SingleCellExperiment_1.22.0 SummarizedExperiment_1.30.2\n [9] Biobase_2.60.0              GenomicRanges_1.52.1       \n[11] GenomeInfoDb_1.36.4         IRanges_2.34.1             \n[13] S4Vectors_0.38.2            BiocGenerics_0.46.0        \n[15] MatrixGenerics_1.12.3       matrixStats_1.0.0          \n\nloaded via a namespace (and not attached):\n  [1] rstudioapi_0.15.0             jsonlite_1.8.7               \n  [3] magrittr_2.0.3                ggbeeswarm_0.7.2             \n  [5] GenomicFeatures_1.52.2        farver_2.1.1                 \n  [7] rmarkdown_2.25                BiocIO_1.10.0                \n  [9] zlibbioc_1.46.0               vctrs_0.6.4                  \n [11] memoise_2.0.1                 Rsamtools_2.16.0             \n [13] DelayedMatrixStats_1.22.6     RCurl_1.98-1.12              \n [15] htmltools_0.5.6.1             S4Arrays_1.0.6               \n [17] progress_1.2.2                AnnotationHub_3.8.0          \n [19] curl_5.1.0                    BiocNeighbors_1.18.0         \n [21] htmlwidgets_1.6.2             cachem_1.0.8                 \n [23] GenomicAlignments_1.36.0      igraph_1.5.1                 \n [25] mime_0.12                     lifecycle_1.0.3              \n [27] pkgconfig_2.0.3               rsvd_1.0.5                   \n [29] Matrix_1.6-1.1                R6_2.5.1                     \n [31] fastmap_1.1.1                 GenomeInfoDbData_1.2.10      \n [33] shiny_1.7.5.1                 digest_0.6.33                \n [35] colorspace_2.1-0              AnnotationDbi_1.62.2         \n [37] dqrng_0.3.1                   irlba_2.3.5.1                \n [39] ExperimentHub_2.8.1           RSQLite_2.3.1                \n [41] beachmat_2.16.0               labeling_0.4.3               \n [43] filelock_1.0.2                fansi_1.0.5                  \n [45] httr_1.4.7                    abind_1.4-5                  \n [47] compiler_4.3.1                bit64_4.0.5                  \n [49] withr_2.5.2                   BiocParallel_1.34.2          \n [51] viridis_0.6.4                 DBI_1.1.3                    \n [53] biomaRt_2.56.1                rappdirs_0.3.3               \n [55] DelayedArray_0.26.7           rjson_0.2.21                 \n [57] tools_4.3.1                   vipor_0.4.5                  \n [59] beeswarm_0.4.0                interactiveDisplayBase_1.38.0\n [61] httpuv_1.6.12                 glue_1.6.2                   \n [63] restfulr_0.0.15               promises_1.2.1               \n [65] grid_4.3.1                    cluster_2.1.4                \n [67] generics_0.1.3                gtable_0.3.4                 \n [69] ensembldb_2.24.1              hms_1.1.3                    \n [71] metapod_1.7.0                 BiocSingular_1.16.0          \n [73] ScaledMatrix_1.8.1            xml2_1.3.5                   \n [75] utf8_1.2.4                    XVector_0.40.0               \n [77] RcppAnnoy_0.0.21              ggrepel_0.9.4                \n [79] BiocVersion_3.17.1            pillar_1.9.0                 \n [81] stringr_1.5.0                 limma_3.56.2                 \n [83] later_1.3.1                   dplyr_1.1.3                  \n [85] BiocFileCache_2.8.0           lattice_0.22-5               \n [87] rtracklayer_1.60.1            bit_4.0.5                    \n [89] tidyselect_1.2.0              locfit_1.5-9.8               \n [91] Biostrings_2.68.1             knitr_1.44                   \n [93] gridExtra_2.3                 ProtGenerics_1.32.0          \n [95] edgeR_3.42.4                  xfun_0.40                    \n [97] statmod_1.5.0                 stringi_1.7.12               \n [99] lazyeval_0.2.2                yaml_2.3.7                   \n[101] evaluate_0.22                 codetools_0.2-19             \n[103] tibble_3.2.1                  BiocManager_1.30.22          \n[105] cli_3.6.1                     uwot_0.1.16                  \n[107] xtable_1.8-4                  munsell_0.5.0                \n[109] Rcpp_1.0.11                   dbplyr_2.3.4                 \n[111] png_0.1-8                     XML_3.99-0.15                \n[113] parallel_4.3.1                ellipsis_0.3.2               \n[115] blob_1.2.4                    prettyunits_1.2.0            \n[117] AnnotationFilter_1.24.0       sparseMatrixStats_1.12.2     \n[119] bitops_1.0-7                  viridisLite_0.4.2            \n[121] scales_1.2.1                  purrr_1.0.2                  \n[123] crayon_1.5.2                  rlang_1.1.1                  \n[125] KEGGREST_1.40.1"
  },
  {
    "objectID": "exercises-04.html#overview",
    "href": "exercises-04.html#overview",
    "title": "7  Workshop",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nThe goal of this workshop is to build a workflow with some example single-cell RNA-seq data."
  },
  {
    "objectID": "exercises-04.html#data",
    "href": "exercises-04.html#data",
    "title": "7  Workshop",
    "section": "7.2 Data",
    "text": "7.2 Data\nThe scRNAseq package provides convenient access to several publicly available data sets in the form of SingleCellExperiment objects. The focus of this package is to capture datasets that are not easily read into R with a one-liner from, e.g., read_csv(). Instead, the necessary data munging is already done so that users only need to call a single function to obtain a well-formed SingleCellExperiment.\n\nlibrary(scRNAseq)\n\nTo see the list of available datasets, use the listDatasets() function:\n\nout &lt;- listDatasets() \nout\n\nDataFrame with 61 rows and 5 columns\n                 Reference  Taxonomy               Part    Number\n               &lt;character&gt; &lt;integer&gt;        &lt;character&gt; &lt;integer&gt;\n1   @aztekin2019identifi..      8355               tail     13199\n2   @bach2017differentia..     10090      mammary gland     25806\n3           @bacher2020low      9606            T cells    104417\n4     @baron2016singlecell      9606           pancreas      8569\n5     @baron2016singlecell     10090           pancreas      1886\n...                    ...       ...                ...       ...\n57    @zeisel2018molecular     10090     nervous system    160796\n58     @zhao2020singlecell      9606 liver immune cells     68100\n59    @zhong2018singlecell      9606  prefrontal cortex      2394\n60  @zilionis2019singlec..      9606               lung    173954\n61  @zilionis2019singlec..     10090               lung     17549\n                      Call\n               &lt;character&gt;\n1        AztekinTailData()\n2        BachMammaryData()\n3        BacherTCellData()\n4   BaronPancreasData('h..\n5   BaronPancreasData('m..\n...                    ...\n57     ZeiselNervousData()\n58   ZhaoImmuneLiverData()\n59   ZhongPrefrontalData()\n60      ZilionisLungData()\n61  ZilionisLungData('mo..\n\n\nYou can load a dataset the following way:\n\nsce &lt;- ZeiselBrainData()\nsce\n\nclass: SingleCellExperiment \ndim: 20006 3005 \nmetadata(0):\nassays(1): counts\nrownames(20006): Tspan12 Tshz1 ... mt-Rnr1 mt-Nd4l\nrowData names(1): featureType\ncolnames(3005): 1772071015_C02 1772071017_G12 ... 1772066098_A12\n  1772058148_F03\ncolData names(10): tissue group # ... level1class level2class\nreducedDimNames(0):\nmainExpName: endogenous\naltExpNames(2): ERCC repeat"
  },
  {
    "objectID": "exercises-04.html#tasks",
    "href": "exercises-04.html#tasks",
    "title": "7  Workshop",
    "section": "7.3 Tasks",
    "text": "7.3 Tasks\n\nPick a scRNA-seq dataset that has more than 5,000 cells and load the SingleCellExperiment (or sce) object.\nShow the number of number of genes and number of observations in the sce object.\nUsing the material we learned in the lecture, analyze the scRNA-seq data using the Biocondutor packages we learned about. This should include (but not be limited to)\n\nQuality control (you must use at least two different QC metrics)\nNormalization\nFeature selection using highly variable genes\nDimensionality reduction using PCA\nData visualization using tSNE or UMAP\nUnsupervised clustering (your choice of method!)\n\nAt the end of your analysis, show a plot of both (i) the PCA plot and (ii) either the tSNE or UMAP plot with the colors represented by the predicted labels from the clustering algorithm.\nFor each component described in Task #3, write 3-4 sentences naming and describing the idea behind the methodology you used, along with interpreting the output.\n\n\n# Add your solution here\n\n\n7.3.1 Useful tips\n\nIf the original dataset was not provided with Ensembl annotation, we can map the identifiers with ensembl=TRUE. Any genes without a corresponding Ensembl identifier is discarded from the dataset.\n\n\nsce &lt;- ZeiselBrainData(ensembl=TRUE)\n\nWarning: Unable to map 1565 of 20006 requested IDs.\n\nhead(rownames(sce))\n\n[1] \"ENSMUSG00000029669\" \"ENSMUSG00000046982\" \"ENSMUSG00000039735\"\n[4] \"ENSMUSG00000033453\" \"ENSMUSG00000046798\" \"ENSMUSG00000034009\""
  }
]